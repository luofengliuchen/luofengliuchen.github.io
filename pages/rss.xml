<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>落风的博客</title>
    <link>http://www.liuschen.com</link>
    <description>落风的博客</description>
    
      <item>
        <title>c++学习笔记</title>
        <link>http://www.liuschen.com/2016/12/01/c++-copy.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/12/01/c++-copy.html</guid>
        <pubDate>Thu, 01 Dec 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;在网上看到，正好正在学习c++，摘抄下比较在意的点,方便再看&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://man.lupaworld.com/content/develop/c&amp;amp;c++/c/c.htm#_Toc520634028&quot;&gt;来源：http://man.lupaworld.com/content/develop/c&amp;amp;c++/c/c.htm#_Toc520634028&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;1.引用与指针的比较&lt;/h1&gt;

&lt;p&gt;引用是C++中的概念，初学者容易把引用和指针混淆一起。一下程序中，n是m的一个引用（reference），m是被引用物（referent）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int m;
int &amp;amp;n = m; n相当于m的别名（绰号），对n的任何操作就是对m的操作。例如有人名叫王小毛，他的绰号是“三毛”。说“三毛”怎么怎么的，其实就是对王小毛说三道四。所以n既不是m的拷贝，也不是指向m的指针，其实n就是m它自己。 引用的一些规则如下： （1）引用被创建的同时必须被初始化（指针则可以在任何时候被初始化）。 （2）不能有NULL引用，引用必须与合法的存储单元关联（指针则可以是NULL）。 （3）一旦引用被初始化，就不能改变引用的关系（指针则可以随时改变所指的对象）。

以下示例程序中，k被初始化为i的引用。语句k = j并不能将k修改成为j的引用，只是把k的值改变成为6。由于k是i的引用，所以i的值也变成了6。

 int i = 5;
int j = 6;
int &amp;amp;k = i;
k = j;    // k和i的值都变成了6;   
上面的程序看起来象在玩文字游戏，没有体现出引用的价值。引用的主要功能是传递函数的参数和返回值。C++语言中，函数的参数和返回值的传递方式有三种：值传递、指针传递和引用传递。
以下是“值传递”的示例程序。由于Func1函数体内的x是外部变量n的一份拷贝，改变x的值不会影响n, 所以n的值仍然是0。
void Func1(int x)
{
	x = x + 10;
}
…
int n = 0;
Func1(n);
cout &amp;lt;&amp;lt; “n = ” &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl;    // n = 0
   
以下是“指针传递”的示例程序。由于Func2函数体内的x是指向外部变量n的指针，改变该指针的内容将导致n的值改变，所以n的值成为10。
    void Func2(int *x)
{
	(* x) = (* x) + 10;
}
…
int n = 0;
Func2(&amp;amp;n);
cout &amp;lt;&amp;lt; “n = ” &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl;        // n = 10
 
以下是“引用传递”的示例程序。由于Func3函数体内的x是外部变量n的引用，x和n是同一个东西，改变x等于改变n，所以n的值成为10。
void Func3(int &amp;amp;x)
{
	x = x + 10;
}
…
int n = 0;
Func3(n);
cout &amp;lt;&amp;lt; “n = ” &amp;lt;&amp;lt; n &amp;lt;&amp;lt; endl;      // n = 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对比上述三个示例程序，会发现“引用传递”的性质象“指针传递”，而书写方式象“值传递”。实际上“引用”可以做的任何事情“指针”也都能够做，为什么还要“引用”这东西？&lt;br /&gt;
答案是“用适当的工具做恰如其分的工作”。&lt;br /&gt;
指针能够毫无约束地操作内存中的如何东西，尽管指针功能强大，但是非常危险。就象一把刀，它可以用来砍树、裁纸、修指甲、理发等等，谁敢这样用？&lt;br /&gt;
如果的确只需要借用一下某个对象的“别名”，那么就用“引用”，而不要用“指针”，以免发生意外。比如说，某人需要一份证明，本来在文件上盖上公章的印子就行了，如果把取公章的钥匙交给他，那么他就获得了不该有的权利。&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2.指针与数组的对比&lt;/h1&gt;

&lt;p&gt;C++/C程序中，指针和数组在不少地方可以相互替换着用，让人产生一种错觉，以为两者是等价的。&lt;/p&gt;

&lt;p&gt;数组要么在静态存储区被创建（如全局数组），要么在栈上被创建。数组名对应着（而不是指向）一块内存，其地址与容量在生命期内保持不变，只有数组的内容可以改变。&lt;/p&gt;

&lt;p&gt;指针可以随时指向任意类型的内存块，它的特征是“可变”，所以我们常用指针来操作动态内存。指针远比数组灵活，但也更危险。&lt;br /&gt;
下面以字符串为例比较指针与数组的特性。&lt;/p&gt;

&lt;p&gt;以下示例中，字符数组a的容量是6个字符，其内容为hello\0。a的内容可以改变，如a[0]= ‘X’。指针p指向常量字符串“world”（位于静态存储区，内容为world\0），常量字符串的内容是不可以被修改的。从语法上看，编译器并不觉得语句p[0]= ‘X’有什么不妥，但是该语句企图修改常量字符串的内容而导致运行错误。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char a[] = “hello”;
a[0] = ‘X’;
cout &amp;lt;&amp;lt; a &amp;lt;&amp;lt; endl;
char *p = “world”;     // 注意p指向常量字符串
p[0] = ‘X’;             // 编译器不能发现该错误
cout &amp;lt;&amp;lt; p &amp;lt;&amp;lt; endl;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3.计算内存容量&lt;/h1&gt;

&lt;p&gt;用运算符sizeof可以计算出数组的容量（字节数）。示例7-3-3（a）中，sizeof(a)的值是12（注意别忘了’\0’）。指针p指向a，但是sizeof(p)的值却是4。这是因为sizeof(p)得到的是一个指针变量的字节数，相当于sizeof(char*)，而不是p所指的内存容量。C++/C语言没有办法知道指针所指的内存容量，除非在申请内存时记住它。&lt;br /&gt;
注意当数组作为函数的参数进行传递时，该数组自动退化为同类型的指针。示例中，不论数组a的容量是多少，sizeof(a)始终等于sizeof(char *)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char a[] = &quot;hello world&quot;;
char *p  = a;
cout&amp;lt;&amp;lt; sizeof(a) &amp;lt;&amp;lt; endl;   // 12字节
cout&amp;lt;&amp;lt; sizeof(p) &amp;lt;&amp;lt; endl;   // 4字节
计算数组和指针的内存容量
  
void Func(char a[100])
{
    cout&amp;lt;&amp;lt; sizeof(a) &amp;lt;&amp;lt; endl;   // 4字节而不是100字节
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;section-3&quot;&gt;4.指针参数是如何传递内存的？&lt;/h1&gt;

&lt;p&gt;如果函数的参数是一个指针，不要指望用该指针去申请动态内存。示例7-4-1中，Test函数的语句GetMemory(str, 200)并没有使str获得期望的内存，str依旧是NULL，为什么？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void GetMemory(char *p, int num)
{
    p = (char *)malloc(sizeof(char) * num);
}
void Test(void)
{
    char *str = NULL;
    GetMemory(str, 100);    // str 仍然为 NULL 
    strcpy(str, &quot;hello&quot;);   // 运行错误
}
示例7-4-1 试图用指针参数申请动态内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;毛病出在函数GetMemory中。编译器总是要为函数的每个参数制作临时副本，指针参数p的副本是 _p，编译器使 _p = p。如果函数体内的程序修改了_p的内容，就导致参数p的内容作相应的修改。这就是指针可以用作输出参数的原因。在本例中，_p申请了新的内存，只是把_p所指的内存地址改变了，但是p丝毫未变。所以函数GetMemory并不能输出任何东西。事实上，每执行一次GetMemory就会泄露一块内存，因为没有用free释放内存。&lt;br /&gt;
如果非得要用指针参数去申请内存，那么应该改用“指向指针的指针”，见示例7-4-2。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void GetMemory2(char **p, int num)
{
    *p = (char *)malloc(sizeof(char) * num);
}
void Test2(void)
{
    char *str = NULL;
    GetMemory2(&amp;amp;str, 100);  // 注意参数是 &amp;amp;str，而不是str
    strcpy(str, &quot;hello&quot;);  
    cout&amp;lt;&amp;lt; str &amp;lt;&amp;lt; endl;
    free(str); 
}
示例7-4-2用指向指针的指针申请动态内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由于“指向指针的指针”这个概念不容易理解，我们可以用函数返回值来传递动态内存。这种方法更加简单，见示例7-4-3。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *GetMemory3(int num)
{
    char *p = (char *)malloc(sizeof(char) * num);
    return p;
}
void Test3(void)
{
    char *str = NULL;
    str = GetMemory3(100); 
    strcpy(str, &quot;hello&quot;);
    cout&amp;lt;&amp;lt; str &amp;lt;&amp;lt; endl;
    free(str); 
}
示例7-4-3 用函数返回值来传递动态内存
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用函数返回值来传递动态内存这种方法虽然好用，但是常常有人把return语句用错了。这里强调不要用return语句返回指向“栈内存”的指针，因为该内存在函数结束时自动消亡，见示例7-4-4。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *GetString(void)
{
    char p[] = &quot;hello world&quot;;
    return p;   // 编译器将提出警告
}
void Test4(void)
{
char *str = NULL;
str = GetString();  // str 的内容是垃圾
cout&amp;lt;&amp;lt; str &amp;lt;&amp;lt; endl;
}
示例7-4-4 return语句返回指向“栈内存”的指针
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用调试器逐步跟踪Test4，发现执行str = GetString语句后str不再是NULL指针，但是str的内容不是“hello world”而是垃圾。&lt;br /&gt;
如果把示例7-4-4改写成示例7-4-5，会怎么样？&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char *GetString2(void)
{
    char *p = &quot;hello world&quot;;
    return p;
}
void Test5(void)
{
    char *str = NULL;
    str = GetString2();
    cout&amp;lt;&amp;lt; str &amp;lt;&amp;lt; endl;
}
示例7-4-5 return语句返回常量字符串
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;函数Test5运行虽然不会出错，但是函数GetString2的设计概念却是错误的。因为GetString2内的“hello world”是常量字符串，位于静态存储区，它在程序生命期内恒定不变。无论什么时候调用GetString2，它返回的始终是同一个“只读”的内存块。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;5.参数的缺省值&lt;/h1&gt;

&lt;p&gt;有一些参数的值在每次函数调用时都相同，书写这样的语句会使人厌烦。C++语言采用参数的缺省值使书写变得简洁（在编译时，缺省值由编译器自动插入）。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;参数缺省值的使用规则：&lt;/h2&gt;

&lt;h3 id=&quot;section-6&quot;&gt;5.1.参数缺省值只能出现在函数的声明中，而不能出现在定义体中。&lt;/h3&gt;
&lt;p&gt;例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void Foo(int x=0, int y=0);    // 正确，缺省值出现在函数的声明中
 
void Foo(int x=0, int y=0)        // 错误，缺省值出现在函数的定义体中
{
…
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;为什么会这样？我想是有两个原因：一是函数的实现（定义）本来就与参数是否有缺省值无关，所以没有必要让缺省值出现在函数的定义体中。二是参数的缺省值可能会改动，显然修改函数的声明比修改函数的定义要方便。&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;5.2.如果函数有多个参数，参数只能从后向前挨个儿缺省，否则将导致函数调用语句怪模怪样。&lt;/h3&gt;
&lt;p&gt;正确的示例如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void Foo(int x, int y=0, int z=0);
错误的示例如下：
void Foo(int x=0, int y, int z=0);   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要注意，使用参数的缺省值并没有赋予函数新的功能，仅仅是使书写变得简洁一些。它可能会提高函数的易用性，但是也可能会降低函数的可理解性。所以我们只能适当地使用参数的缺省值，要防止使用不当产生负面效果。示例8-3-2中，不合理地使用参数的缺省值将导致重载函数output产生二义性。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream.h&amp;gt;
void output( int x);
void output( int x, float y=0.0);
 
void output( int x)
{
    cout &amp;lt;&amp;lt; &quot; output int &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; endl ;
}
 
void output( int x, float y)
{
    cout &amp;lt;&amp;lt; &quot; output int &quot; &amp;lt;&amp;lt; x &amp;lt;&amp;lt; &quot; and float &quot; &amp;lt;&amp;lt; y &amp;lt;&amp;lt; endl ;
}
 
void main(void)
{
    int x=1;
    float y=0.5;
//  output(x);          // error! ambiguous call
    output(x,y);        // output int 1 and float 0.5
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数的缺省值将导致重载函数产生二义性&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>c语言微知识点</title>
        <link>http://www.liuschen.com/2016/11/27/C++-point.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/27/C++-point.html</guid>
        <pubDate>Sun, 27 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;c-new&quot;&gt;1.c++ 中如何new一个对象&lt;/h2&gt;

&lt;p&gt;demo* demo0 = new demo();&lt;/p&gt;

&lt;h2 id=&quot;c-&quot;&gt;2.c语言结构体.和-&amp;gt;的区别&lt;/h2&gt;

&lt;p&gt;结构体变量的引用和赋值，有两种形式，一种是用符号”.”，一种是用符号”-&amp;gt;”。结构体指针变量一般用”-&amp;gt;”，非结构体指针变量，也就是一般结构体变量，一般用”.”。&lt;/p&gt;

&lt;h2 id=&quot;c&quot;&gt;3.C++回调函数&lt;/h2&gt;

&lt;p&gt;1.重新定义一个函数指针类型&lt;/p&gt;

&lt;p&gt;typedef void(*pcb)();&lt;/p&gt;

&lt;p&gt;2.将该函数指针作为函数的形参&lt;/p&gt;

&lt;p&gt;virtual void getSomeThing(pcb call);&lt;/p&gt;

&lt;p&gt;3.传入回掉函数&lt;/p&gt;

&lt;p&gt;4.在getSomeThing中执行回调函数call()&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void Func(char *s)；// 函数原型
void (*pFunc) (char *);//函数指针
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;4.&amp;amp;运算符&lt;/h2&gt;

&lt;p&gt;1.求址运算符&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int *p;
int a = 9;
p = &amp;amp;a;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.引用（相当于别名,用作引用的名字不能作为其他变量别名）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int i = 5;
int j = 6;
int &amp;amp;k = i;
k = j;
&lt;/code&gt;&lt;/pre&gt;

</description>
      </item>
    
      <item>
        <title>jni编程中C++/C语言一些问题</title>
        <link>http://www.liuschen.com/2016/11/20/C++-jni.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/20/C++-jni.html</guid>
        <pubDate>Sun, 20 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;uint8t&quot;&gt;1.uint8_t&lt;/h2&gt;
&lt;p&gt;首先，先来看一下jni.h关于j数据类型的定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#ifdef HAVE_INTTYPES_H
# include &amp;lt;inttypes.h&amp;gt;      /* C99 */
typedef uint8_t         jboolean;       /* unsigned 8 bits */
typedef int8_t          jbyte;          /* signed 8 bits */
typedef uint16_t        jchar;          /* unsigned 16 bits */
typedef int16_t         jshort;         /* signed 16 bits */
typedef int32_t         jint;           /* signed 32 bits */
typedef int64_t         jlong;          /* signed 64 bits */
typedef float           jfloat;         /* 32-bit IEEE 754 */
typedef double          jdouble;        /* 64-bit IEEE 754 */
#else
typedef unsigned char   jboolean;       /* unsigned 8 bits */
typedef signed char     jbyte;          /* signed 8 bits */
typedef unsigned short  jchar;          /* unsigned 16 bits */
typedef short           jshort;         /* signed 16 bits */
typedef int             jint;           /* signed 32 bits */
typedef long long       jlong;          /* signed 64 bits */
typedef float           jfloat;         /* 32-bit IEEE 754 */
typedef double          jdouble;        /* 64-bit IEEE 754 */
#endif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而在inttypes.h中又有如下定义，当然不同环境可能定义略有不同&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#if defined(_CHAR_IS_SIGNED)  
typedef char                    int8_t;  
#else  
#if defined(__STDC__)  
typedef signed char             int8_t;  
#endif  
#endif  
typedef short                   int16_t;  
typedef int                     int32_t;  
#ifdef  _LP64  
#define _INT64_TYPE  
typedef long                    int64_t;  
#else   /* _ILP32 */  
#if defined(_LONGLONG_TYPE)  
#define _INT64_TYPE  
typedef long long               int64_t;  
#endif  
#endif  
  
typedef unsigned char           uint8_t;  
typedef unsigned short          uint16_t;  
typedef unsigned int            uint32_t;  
#ifdef  _LP64  
typedef unsigned long           uint64_t;  
#else   /* _ILP32 */  
#if defined(_LONGLONG_TYPE)  
typedef unsigned long long      uint64_t;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如uint8_t是unsigned char的别名，能直观的体现所表示类型的字节数，以及符号属性，在不同的环境即是所表示的数据类型不同，但是只要在inttypes.h中将这种差异处理了，程序通过调用uint8_t并不会有什么影响，因为它只表示无符号一字节的数据，方便了程序不同平台的移植。在做音视频传输时，帧的数据就是通过uint8_t数组来存储的。&lt;/p&gt;

&lt;h2 id=&quot;env&quot;&gt;2.env&lt;/h2&gt;
&lt;p&gt;注意：jni.h头文件中对于&lt;em&gt;**.c &amp;amp; **&lt;/em&gt;.cpp采用不同的定义&lt;br /&gt;
在C的定义中,env是一个两级指针,而在C++的定义中,env是个一级指针&lt;br /&gt;
C形式需要对env指针进行双重deferencing，而且须将env作为第一个参数传给jni函数,如进行字符串转化时：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;C++:env0-&amp;gt;GetStringUTFChars(url, NULL)
C:(*env)-&amp;gt;GetStringUTFChars(env,url, NULL)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;3.类型转换&lt;/h2&gt;

&lt;p&gt;C++ char const* 类型和字符串互相转化&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;
std::string src1(rootpath);
std::string src2(streamName0);
std::string dest = src1 + src2;
char const* streamName = dest.c_str();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;cuintt&quot;&gt;4.C++将uint_t数组输出到文件&lt;/h2&gt;

&lt;p&gt;头文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;fstream&amp;gt;
using namespace std;//这个必须加
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ofstream outfile;
outfile.open(store_path);
for(i=0;i&amp;lt;size;i++){
	outfile&amp;lt;&amp;lt;fReceiveBuffer[i];
}
outfile.close();
&lt;/code&gt;&lt;/pre&gt;
</description>
      </item>
    
      <item>
        <title>RTMP延时问题</title>
        <link>http://www.liuschen.com/2016/11/17/RTMP.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/17/RTMP.html</guid>
        <pubDate>Thu, 17 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;用的是方案一的流媒体传输，如果只传输视频，延时非常严重。分析原因无非两个，一个是推流客户端的问题，一个是播放器的问题。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在window上收流时发现，延时主要发生在两个函数上&lt;/p&gt;

&lt;h1 id=&quot;avformatopeninput&quot;&gt;1.avformat_open_input&lt;/h1&gt;

&lt;h1 id=&quot;avformatfindstreaminfo&quot;&gt;2.avformat_find_stream_info&lt;/h1&gt;

&lt;p&gt;这个是用来解析流信息的，可以在之前设置这个&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifmt_ctx-&amp;gt;probesize = 100 * 1024;
ifmt_ctx-&amp;gt;max_analyze_duration = 5 * AV_TIME_BASE;

if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) &amp;lt; 0) {
	printf( &quot;Failed to retrieve input stream information&quot;);
	goto end;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以大大提高解析速度&lt;/p&gt;

&lt;p&gt;其实用ffplay播放时，通过&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-fflags nobuffer -analyzeduration 1000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参数已经解决了这个问题，但是播放延迟还在，那么就有可能是客户端的问题了，之前是通过&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	recorder = new FFmpegFrameRecorder(url, width, height, 1);
    recorder.setVideoBitrate(2000000);
    recorder.setVideoCodec(avcodec.AV_CODEC_ID_H264);
    recorder.setFormat(&quot;flv&quot;);
    recorder.setFrameRate(10);
    recorder.setVideoOption(&quot;tune&quot;,&quot;zerolatency&quot;);
    recorder.setVideoOption(&quot;preset&quot;,&quot;ultrafast&quot;);
    recorder.setGopSize(GOP_LENGTH_IN_FRAMES);
    recorder.setVideoQuality(28);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;单纯设置了视频的参数，但是通过阅读FFmpegFrameRecorder源码，发现音视频参数都设置了初始值，所以可能是默认仍然传输了音频。所以添加了&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    recorder.setAudioBitrate(0);
    recorder.setAudioChannels(-1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;果然本来延时10多秒，恢复到了正常范围，延迟大约1s左右。&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>ffmpeg/ffplay命令记录（转载，整理）</title>
        <link>http://www.liuschen.com/2016/11/16/ffplay.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/16/ffplay.html</guid>
        <pubDate>Wed, 16 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;打出来的视频信息，但是没有播放，但是会输出视频帧到当前目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i &quot;rtmp://192.168.24.142/oflaDemo/st6 live=1&quot; -f image2 -vf fps=fps=1 out%d.png
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;播放RMTP直播流&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffplay -i &quot;rtmp://192.168.24.142/oflaDemo/st6 live=1&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;fplay播放yuv文件命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffplay -f rawvideo -video_size 1920x1080 a.yuv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置播放没有缓冲区&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffplay -x 240 -y 320 109.flv -fflags nobuffer


ffplay -i &quot;rtmp://192.168.24.142/oflaDemo/st5 live=1&quot; -fflags nobuffer -analyzeduration 1000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;列出本机可用设备&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -list_devices true -f dshow -i dummy 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;播放本机USB摄像头捕捉数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffplay -f dshow -i video=&quot;USB Camera&quot; -f dshow -i audio=&quot;音频设备&quot;
ffplay -f vfwcap -i 0
//设置分辨率和帧率播放
ffplay -s 424x240 -r 5 -f dshow -i video=&quot;USB Camera&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将USB摄像头捕捉的数据编码成H264保存到文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -f dshow -i video=&quot;USB Camera&quot; -vcodec libx264 camera.mkv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将USB摄像头数据通过RTMP协议发送出去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -f dshow -i video=&quot;USB Camera&quot; -vcodec libx264 -preset:v ultrafast -tune:v zerolatency -f flv rtmp://localhost/oflaDemo/livestream
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看设备选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -list_options true -f dshow -i video=&quot;USB Camera&quot; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分离视频音频流&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i input_file -vcodec copy -an output_file_video　　//分离视频流
ffmpeg -i input_file -acodec copy -vn output_file_audio　　//分离音频流
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;视频解复用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –i test.mp4 –vcodec copy –an –f m4v test.264
ffmpeg –i test.avi –vcodec copy –an –f m4v test.264
//这个m4v是表示mp4的格式，如果是flv就需要写成flv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;视频转码&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –i test.mp4 –vcodec h264 –s 352*278 –an –f m4v test.264              //转码为码流原始文件
ffmpeg –i test.mp4 –vcodec h264 –bf 0 –g 25 –s 352*278 –an –f m4v test.264  //转码为码流原始文件
ffmpeg –i test.avi -vcodec mpeg4 –vtag xvid –qsame test_xvid.avi            //转码为封装文件
//-bf B帧数目控制，-g 关键帧间隔控制，-s 分辨率控制
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;视频封装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –i video_file –i audio_file –vcodec copy –acodec copy output_file
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;视频剪切&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –i test.avi –r 1 –f image2 image-%3d.jpeg        //提取图片
ffmpeg -ss 0:1:30 -t 0:0:20 -i input.avi -vcodec copy -acodec copy output.avi    //剪切视频
//-r 提取图像的频率，-ss 开始时间，-t 持续时间
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;视频录制&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –i rtsp://192.168.3.205:5555/test –vcodec copy out.avi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YUV序列播放&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffplay -f rawvideo -video_size 1920x1080 input.yuv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;YUV序列转AVI&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg –s w*h –pix_fmt yuv420p –i input.yuv –vcodec mpeg4 output.avi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;采集usb摄像头视频命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -t 20 -f vfwcap -i 0 -r 8 -f mp4 cap1111.mp4

具体说明如下：我们采集10秒，采集设备为vfwcap类型设备，第0个vfwcap采集设备（如果系统有多个vfw的视频采集设备，可以通过-i num来选择），每秒8帧，输出方式为文件，格式为mp4。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最简单的抓屏：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -f gdigrab -i desktop out.mpg 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从屏幕的（10,20）点处开始，抓取640x480的屏幕，设定帧率为5 ：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -f gdigrab -framerate 5 -offset_x 10 -offset_y 20 -video_size 640x480 -i desktop out.mpg 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ffmpeg从视频中生成gif图片：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i capx.mp4 -t 10 -s 320x240 -pix_fmt rgb24 jidu1.gif
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ffmpeg将图片转换为视频&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;指定编码，帧率和码率，图片在当前目录下需要按照自然数排序命名
ffmpeg -f image2 -i c:\temp\d.jpg -vcodec libx264 -r 10 -b 200k  test.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将文件当做直播送至live&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -re -i localFile.mp4 -c copy -f flv rtmp://server/live/streamName
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将直播媒体保存至本地文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i rtmp://server/live/streamName -c copy dump.flv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其中一个直播流，视频改用h264压缩，音频不变，送至另外一个直播服务流&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v libx264 -vpre slow -f flv rtmp://server/live/h264Stream
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其中一个直播流，视频改用h264压缩，音频改用faac压缩，送至另外一个直播服务流&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i rtmp://server/live/originalStream -c:a libfaac -ar 44100 -ab 48k -c:v libx264 -vpre slow -vpre baseline -f flv rtmp://server/live/h264Stream
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将其中一个直播流，视频不变，音频改用faac压缩，送至另外一个直播服务流&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i rtmp://server/live/originalStream -acodec libfaac -ar 44100 -ab 48k -vcodec copy -f flv rtmp://server/live/h264_AAC_Stream
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将一个高清流，复制为几个不同视频清晰度的流重新发布，其中音频不变&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -re -i rtmp://server/live/high_FMLE_stream -acodec copy -vcodec x264lib -s 640×360 -b 500k -vpre medium -vpre baseline rtmp://server/live/baseline_500k -acodec copy -vcodec x264lib -s 480×272 -b 300k -vpre medium -vpre baseline rtmp://server/live/baseline_300k -acodec copy -vcodec x264lib -s 320×200 -b 150k -vpre medium -vpre baseline rtmp://server/live/baseline_150k -acodec libfaac -vn -ab 48k rtmp://server/live/audio_only_AAC_48k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;功能一样，只是采用-x264opts选项&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -re -i rtmp://server/live/high_FMLE_stream -c:a copy -c:v x264lib -s 640×360 -x264opts bitrate=500:profile=baseline:preset=slow rtmp://server/live/baseline_500k -c:a copy -c:v x264lib -s 480×272 -x264opts bitrate=300:profile=baseline:preset=slow rtmp://server/live/baseline_300k -c:a copy -c:v x264lib -s 320×200 -x264opts bitrate=150:profile=baseline:preset=slow rtmp://server/live/baseline_150k -c:a libfaac -vn -b:a 48k rtmp://server/live/audio_only_AAC_48k
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将当前摄像头及音频通过DSSHOW采集，视频h264、音频faac压缩后发布&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -r 25 -f dshow -s 640×480 -i video=”video source name”:audio=”audio source name” -vcodec libx264 -b 600k -vpre slow -acodec libfaac -ab 128k -f flv rtmp://server/application/stream_name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将一个JPG图片经过h264压缩循环输出为mp4视频&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg.exe -i INPUT.jpg -an -vcodec libx264 -coder 1 -flags +loop -cmp +chroma -subq 10 -qcomp 0.6 -qmin 10 -qmax 51 -qdiff 4 -flags2 +dct8x8 -trellis 2 -partitions +parti8x8+parti4x4 -crf 24 -threads 0 -r 25 -g 25 -y OUTPUT.mp4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将普通流视频改用h264压缩，音频不变，送至高清流服务(新版本FMS live=1)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v libx264 -vpre slow -f flv “rtmp://server/live/h264Stream live=1〃
&lt;/code&gt;&lt;/pre&gt;
</description>
      </item>
    
      <item>
        <title>Android数据库并发操作</title>
        <link>http://www.liuschen.com/2016/11/09/database.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/09/database.html</guid>
        <pubDate>Wed, 09 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;android-sqlite&quot;&gt;1.Android SQLite函数调用&lt;/h1&gt;

&lt;p&gt;首先在SQLiteDatabase中有已经封装好的增删改查操作函数，操作简单数据自己在封装一个dao直接调用即可,操作复杂可以自定义SQL语句，但是里面都是通过SQLiteStatement来执行其对应操作。查询操作执行和其他不同，是通过SQLiteCursorDriver来找到curser的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;查询(读):外层包装-&amp;gt;SQL语句-&amp;gt;SQLiteCursorDriver
增删改(写)：外层包装-&amp;gt;SQL语句-&amp;gt;SQLiteStatement
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;所以可以直接如网上所说，用来SQLiteStatement写，将&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQLiteDatabase database = new SQLiteDatabase();  
if (database.isOpen())   
{  
    database.beginTransaction();  
    try {  
         //sql为insert into tableName (name) values (&quot;test&quot;)  
        database.execSQL(sql);  
    }  
        database.setTransactionSuccessful();  
    } finally {  
        database.endTransaction();  
    }  
    database.close();  
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SQLiteDatabase database = new SQLiteDatabase();  
//sql为insert into tableName (name) values (?)  
SQLiteStatement sqlListStatment = database.compileStatement(sql);  
if (database.isOpen())   
{  
    database.beginTransaction();  
    try {  
        //index 为1开始索引，value为入库的值  
        //bingXXX为插入XXX类型
         sqLiteStatement.bindString(index, value);  
         sqLiteStatement.executeInsert();  
    }  
        database.setTransactionSuccessful();  
    } finally {  
        database.endTransaction();  
    }  
    database.close();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样性能应该会提升不少，因为SQLiteDatabase封装的时候做了一些检查操作，每一条省下的时间可能微不足道，但如果数据过万就不同了。&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;2.事务操作&lt;/h1&gt;

&lt;p&gt;Android SQL语句默认一条语句是一个事务，事务是为了保证操作的安全性，但是事务实际执行却会耗费大量资源，如果反复执行一个插入操作10000次将会相当耗时，所以在反复执行增删改操所时，一定在外层加入事务，让整个任务作为一条事务来执行。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	db.beginTransaction();
	try{
		//批量写操作
	}catch(Exception e){
		e.printStackTrace();
	}finally{
		db.endTransaction();
	}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;sqlite-database-is-locked&quot;&gt;3.SQLite database is locked&lt;/h1&gt;

&lt;p&gt;这个是由于同时操作数据库抛出的异常，数据库同一时间不能被并发访问，这一点用单例模式可以解决，SQLiteOpenHelper在多个线程创建多个对象不能同时读和写，只能同时读，但是用单例模式，单个SQLiteOpenHelper对象在不同线程可以同时读写。&lt;/p&gt;

&lt;p&gt;关于SQLiteOpenHelper用单例在不同线程同时执行读写，写写操作，我做了一些测试，发现数据库的访问是顺序执行的，两个线程同时进行写操作，先执行的会先操作，另一个线程就阻塞了，等到先执行的操作完成，另一个线程才开始操作。&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>SDL&android</title>
        <link>http://www.liuschen.com/2016/11/06/sdl.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/11/06/sdl.html</guid>
        <pubDate>Sun, 06 Nov 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;下载SDL:&lt;a href=&quot;http://www.libsdl.org/download-2.0.php&quot;&gt;http://www.libsdl.org/download-2.0.php&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;jnisdl&quot;&gt;1.通过jni编译SDL&lt;/h1&gt;

&lt;p&gt;解压下载的SDL文件&lt;/p&gt;

&lt;p&gt;将目录下的include，src文件夹以及Android.mk文件分别移动到jni目录下，执行ndk-build&lt;/p&gt;

&lt;p&gt;我用的版本是2.0.5,编译会出现如下错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[armeabi] Compile thumb  : SDL2 &amp;lt;= SDL_render_gles.c
[armeabi] Compile thumb  : SDL2 &amp;lt;= SDL_render_gles2.c
D:/project/AndroidSDL/app/src/main/jni/src/render/opengles2/SDL_gles2funcs.h: In function &#39;GLES2_LoadFunctions&#39;:
D:/project/AndroidSDL/app/src/main/jni/src/render/opengles2/SDL_render_gles2.c:294:45: warning: assignment from incompatible pointer type [enabled by default]
 #define SDL_PROC(ret,func,params) data-&amp;gt;func=func;
                                             ^
D:/project/AndroidSDL/app/src/main/jni/src/render/opengles2/SDL_gles2funcs.h:56:1: note: in expansion of macro &#39;SDL_PROC&#39;
 SDL_PROC(void, glShaderSource, (GLuint, GLsizei, const GLchar* const*, const GLint *))
 ^
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;具体原因没有分析，但是改用2.0.3后，编译通过(&lt;a href=&quot;http://hg.libsdl.org/SDL&quot;&gt;历史下载列表&lt;/a&gt;)。&lt;/p&gt;

&lt;p&gt;编译出so文件后,又将目录下的AndroidProject中的Activity移动到了项目下（路径名也要一致，因为在c中有映射，改那个太麻烦），运行工程，再次出错&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JNI DETECTED ERROR IN APPLICATION: JNI GetStaticMethodID called with pending exception &#39;java.lang.NoSuchMethodError&#39; thrown in int org.libsdl.app.SDLActivity.nativeInit(java.lang.Object):-2
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]     in call to GetStaticMethodID
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]     from int org.libsdl.app.SDLActivity.nativeInit(java.lang.Object)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65] &quot;SDLThread&quot; prio=5 tid=19 Runnable
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   | group=&quot;main&quot; sCount=0 dsCount=0 obj=0x12d884c0 self=0xab66f138
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   | sysTid=22530 nice=0 cgrp=default sched=0/0 handle=0xab66f768
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   | state=R schedstat=( 823907 204323 7 ) utm=0 stm=0 core=6 HZ=100
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   | stack=0xe4a25000-0xe4a27000 stackSize=1036KB
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   | held mutexes= &quot;mutator lock&quot;(shared held)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #00 pc 000049f4  /system/lib/libbacktrace_libc++.so (_ZN13UnwindCurrent6UnwindEjP8ucontext+23)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #01 pc 000030cd  /system/lib/libbacktrace_libc++.so (_ZN9Backtrace6UnwindEjP8ucontext+8)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #02 pc 002481c5  /system/lib/libart.so (_ZN3art15DumpNativeStackERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEEiPKcPNS_6mirror9ArtMethodE+68)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #03 pc 0022bf1d  /system/lib/libart.so (_ZNK3art6Thread4DumpERNSt3__113basic_ostreamIcNS1_11char_traitsIcEEEE+144)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #04 pc 000b0c53  /system/lib/libart.so (_ZN3artL8JniAbortEPKcS1_+582)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #05 pc 000b1389  /system/lib/libart.so (_ZN3art9JniAbortFEPKcS1_z+60)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #06 pc 000b44cb  /system/lib/libart.so (_ZN3art11ScopedCheckC2EP7_JNIEnviPKc+1286)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #07 pc 000b6793  /system/lib/libart.so (_ZN3art8CheckJNI17GetStaticMethodIDEP7_JNIEnvP7_jclassPKcS6_+26)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #08 pc 00042707  /data/app/ketr.com.androidsdl-1/lib/arm/libSDL2.so (SDL_Android_Init+106)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #09 pc 0000116f  /data/app/ketr.com.androidsdl-1/lib/arm/libSDL2main.so (Java_org_libsdl_app_SDLActivity_nativeInit+2)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   native: #10 pc 0002d319  /data/data/ketr.com.androidsdl/cache/slice-slice_5-classes.dex (Java_org_libsdl_app_SDLActivity_nativeInit__Ljava_lang_Object_2+100)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   at org.libsdl.app.SDLActivity.nativeInit(Native method)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   at org.libsdl.app.SDLMain.run(SDLActivity.java:1028)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65]   at java.lang.Thread.run(Thread.java:818)
11-08 13:52:18.681 22508-22530/? A/art: art/runtime/check_jni.cc:65] 
11-08 13:52:18.701 22508-22530/? A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x1d in tid 22530 (SDLThread)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个问题是由于jni对java的调用找不到方法导致的，c函数调用Java时，在core/android里面的实现出了问题，里面的函数还用了一些define别名，以及extern C_LINKAGE 来引用一些函数，最后发现调用流程是：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SDLActivity里面调用init的本地方法；&lt;/li&gt;
  &lt;li&gt;init方法的实现是在main/Android文件夹里SDL_Android_main.c里；&lt;/li&gt;
  &lt;li&gt;init方法里面调用了core/android里面的方法初始化了，这个初始化的方法调用的是Android的Java代码；&lt;/li&gt;
  &lt;li&gt;接着又调用了SDL_main方法，这个方式是main方法的别名define定义的；&lt;/li&gt;
  &lt;li&gt;这个main方法就是自己写的主函数&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;也就是说，SDL最终还是调用的Android里面的方法，就变成了这样：&lt;/p&gt;

&lt;p&gt;Android播放器实现（java）——&amp;gt;调用SDL本地方法——&amp;gt;调用java方法——&amp;gt;调用本地引擎方法&lt;/p&gt;

&lt;p&gt;有点拐弯抹角，实现起来并不经济，SDL只是为了跨平台而实现的一层框架，并且里面对于Android中Java方法的相互调用，方法名写的比较死，耦合太大，随着版本的升级也要不断改动，如果不需要跨平台，就完全没用。&lt;/p&gt;

&lt;p&gt;PS:通过javah生成头文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;app\build\intermediates\classes\debug&amp;gt;javah -jni org.libsdl.app.SDLActivity
错误: 无法访问android.app.Activity
  找不到android.app.Activity的类文件
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将目录切换到源文件src目录下执行，能正常生成头文件&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>live555初探-修改live555当前工作目录</title>
        <link>http://www.liuschen.com/2016/10/30/live555-dir.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/30/live555-dir.html</guid>
        <pubDate>Sun, 30 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;我的live555是在virtualstudio上编译的，但是修改工作目录却是为了移植Android平台的便利。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section&quot;&gt;1.输出&lt;/h1&gt;
&lt;p&gt;virtualStudio不太熟悉，所以调试live555也没用里面的工具，老办法，在代码里面输出内容来跟踪代码执行。&lt;/p&gt;

&lt;p&gt;live555是通过UsageEnvironment来执行输出到屏幕的，live555MediaServer里面就可以看到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;*env &amp;lt;&amp;lt; &quot;Play streams from this server using the URL\n\t&quot;
   &amp;lt;&amp;lt; urlPrefix &amp;lt;&amp;lt; &quot;&amp;lt;filename&amp;gt;\nwhere &amp;lt;filename&amp;gt; is a file present in the current directory.\n&quot;;
  	*env &amp;lt;&amp;lt; &quot;Each file&#39;s type is inferred from its name suffix:\n&quot;;
  	*env &amp;lt;&amp;lt; &quot;\t\&quot;.264\&quot; =&amp;gt; a H.264 Video Elementary Stream file\n&quot;;
  	*env &amp;lt;&amp;lt; &quot;\t\&quot;.265\&quot; =&amp;gt; a H.265 Video Elementary Stream file\n&quot;;
  	*env &amp;lt;&amp;lt; &quot;\t\&quot;.aac\&quot; =&amp;gt; an AAC Audio (ADTS format) file\n&quot;;
 	 *env &amp;lt;&amp;lt; &quot;\t\&quot;.ac3\&quot; =&amp;gt; an AC-3 Audio file\n&quot;;
 	 *env &amp;lt;&amp;lt; &quot;\t\&quot;.amr\&quot; =&amp;gt; an AMR Audio file\n&quot;;
  	*env &amp;lt;&amp;lt; &quot;\t\&quot;.dv\&quot; =&amp;gt; a DV Video file\n&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而在直接找不到env时，可以用envir()来输出，envir()是Medium基类的成员函数，许多类都继承于它，返回的也是UsageEnvironment的对象&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;2.查找-修改&lt;/h1&gt;

&lt;p&gt;如果要修改工作目录，也就是存放音视频的目录，首先需要找出打开文件的操作fopen，全局搜索fopen,总共有三处调用&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OuputFile.cpp&lt;/li&gt;
  &lt;li&gt;InputFile,cpp&lt;/li&gt;
  &lt;li&gt;DynamicRTSPServer.cpp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在每次打开文件前都做好输出标记，发现如果通过rtsp请求服务器文件，是只经过DynamicRTSPServer.cpp的,那么就在打开文件操作前添加如下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char const* rootpath = &quot;自定义的根目录&quot;;

envir() &amp;lt;&amp;lt; &quot;beforefilepath&quot; &amp;lt;&amp;lt; streamName0&amp;lt;&amp;lt;&quot;\n&quot;;

std::string src1(rootpath);
//streamName0：函数参数由streamName改为streamName0
std::string src2(streamName0);
std::string dest = src1 + src2;
char const* streamName;
//因为该函数不只被调用一次，不加判断的话，根可能被重复添加
if (strstr(streamName0,rootpath)) {
	streamName = streamName0;
}
else {
	streamName = dest.c_str();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还要添加头文件iostream&lt;/p&gt;

&lt;h1 id=&quot;section-2&quot;&gt;3.进一步修改&lt;/h1&gt;

&lt;p&gt;在RTSPServer.hh中添加声明&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char const* root_path;
void setRootPath(char const* rootpath);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;RTSPServer.cpp中添加实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void RTSPServer::setRootPath(char const* rootpath) {
	root_path = rootpath;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，将子项目mediaServer设为启动项目，重新编译。&lt;/p&gt;

&lt;p&gt;将以上代码中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char const* rootpath = &quot;自定义的根目录&quot;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改为&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;char const* rootpath = RTSPServer::root_path;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;此时可以在live555MediaServer.cpp中直接设置工作目录了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rtspServer-&amp;gt;setRootPath(&quot;D:/music/&quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将修改移动到Android的ndk下编译，可能会出现找不到iostream的错误，需要在Application.mk下添加，加入静态库支持：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;APP_STL := stlport_static
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过Live555MedaiServer新建流媒体服务对象时,其中代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;portNumBits rtspServerPortNum = 554;
   	rtspServer = DynamicRTSPServer::createNew(*env, rtspServerPortNum, authDB);
if (rtspServer == NULL) {
 rtspServerPortNum = 8554;
 rtspServer = DynamicRTSPServer::createNew(*env, rtspServerPortNum, authDB);
   	}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第一次会设置554为访问端口，不成功会改设8554，在Android端曾出现访问不成功，最后发现端口被设成了8554，访问这个端口是需要在地址中具体指明的。&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>live555 window下编译运行以及Android平台的移植</title>
        <link>http://www.liuschen.com/2016/10/29/live555-android.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/29/live555-android.html</guid>
        <pubDate>Sat, 29 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;live555是一个开源的多媒体直播框架,代码是用c++写的,主要用到的直播协议是RTSP，在以下地址中可以直接下载到源码&lt;a href=&quot;http://www.live555.com/liveMedia/public/&quot;&gt;http://www.live555.com/liveMedia/public/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;window&quot;&gt;1.window平台的编译&lt;/h1&gt;

&lt;h2 id=&quot;httpblogcsdnnethjl240articledetails48159243httpblogcsdnnethjl240articledetails48159243&quot;&gt;参考地址：&lt;a href=&quot;http://blog.csdn.net/hjl240/article/details/48159243&quot;&gt;http://blog.csdn.net/hjl240/article/details/48159243&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;在编译的过程中遇到了两个问题，一个是以上博客中提到的，以下单独摘出：&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;错误 1&lt;/h3&gt;

 	error C4996: ‘_snprintf’:This function or variable may be unsafe. Consider using _snprintf_s instead. Todisable deprecation, use _CRT_SECURE_NO_WARNINGS.&lt;br /&gt;

&lt;p&gt;错误是说 使用这个_snprintf函数不安全。&lt;/p&gt;

&lt;p&gt;解决这个错误的方法是更改预处理定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;项目-&amp;gt;属性-&amp;gt;配置属性-&amp;gt;C/C++ -&amp;gt;预处理器 -&amp;gt; 预处理器定义
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;增加： _CRT_SECURE_NO_DEPRECATE&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;错误 2&lt;/h3&gt;

&lt;p&gt;说是一些方法过时了，编译不通过，我用的是一个相对简单的方法:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;项目-&amp;gt;属性-&amp;gt;配置属性-&amp;gt;C/C++ -&amp;gt;常规-&amp;gt;SDL检查 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;改为‘否’就好了&lt;/p&gt;

&lt;h1 id=&quot;android&quot;&gt;2.Android平台的移植&lt;/h1&gt;

&lt;p&gt;熟悉在Androidstudio下jni开发就知道首先在main目录下建一个jni目录，将live555源码中的BaseUsageEnvironment,groupsock,liveMedia,mediaServer和UsageEnvironment文件夹分别拷贝到里面，然后再文件夹下建立一个Android.mk文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

LOCAL_MODULE := liblive555

LOCAL_ARM_MODE := arm

LOCAL_PRELINK_MODULE := false

LOCAL_CPPFLAGS := \
	-DNULL=0 -DSOCKLEN_T=socklen_t -DNO_SSTREAM -DBSD=1 -DNO_SSTREAM -fexceptions -DANDROID -DXLOCALE_NOT_USED

LOCAL_C_INCLUDES := \
$(LOCAL_PATH) \
$(LOCAL_PATH)/BasicUsageEnvironment/include \
$(LOCAL_PATH)/BasicUsageEnvironment \
$(LOCAL_PATH)/UsageEnvironment/include \
$(LOCAL_PATH)/UsageEnvironment \
$(LOCAL_PATH)/groupsock/include \
$(LOCAL_PATH)/groupsock \
$(LOCAL_PATH)/liveMedia/include \
$(LOCAL_PATH)/liveMedia \

LOCAL_MODULE_TAGS := optional

LOCAL_SRC_FILES := \
BasicUsageEnvironment/BasicHashTable.cpp  \
BasicUsageEnvironment/BasicTaskScheduler.cpp  \
BasicUsageEnvironment/BasicTaskScheduler0.cpp  \
BasicUsageEnvironment/BasicUsageEnvironment.cpp  \
BasicUsageEnvironment/BasicUsageEnvironment0.cpp  \
BasicUsageEnvironment/DelayQueue.cpp  \
groupsock/GroupEId.cpp  \
groupsock/Groupsock.cpp  \
groupsock/GroupsockHelper.cpp  \
groupsock/inet.c  \
groupsock/IOHandlers.cpp  \
groupsock/NetAddress.cpp  \
groupsock/NetInterface.cpp  \
liveMedia/AC3AudioFileServerMediaSubsession.cpp  \
liveMedia/AC3AudioRTPSink.cpp  \
liveMedia/AC3AudioRTPSource.cpp  \
liveMedia/AC3AudioStreamFramer.cpp  \
liveMedia/ADTSAudioFileServerMediaSubsession.cpp  \
liveMedia/ADTSAudioFileSource.cpp  \
liveMedia/AMRAudioFileServerMediaSubsession.cpp  \
liveMedia/AMRAudioFileSink.cpp  \
liveMedia/AMRAudioFileSource.cpp  \
liveMedia/AMRAudioRTPSink.cpp  \
liveMedia/AMRAudioRTPSource.cpp  \
liveMedia/AMRAudioSource.cpp  \
liveMedia/AudioInputDevice.cpp  \
liveMedia/AudioRTPSink.cpp  \
liveMedia/AVIFileSink.cpp  \
liveMedia/Base64.cpp  \
liveMedia/BasicUDPSink.cpp  \
liveMedia/BasicUDPSource.cpp  \
liveMedia/BitVector.cpp  \
liveMedia/ByteStreamFileSource.cpp  \
liveMedia/ByteStreamMemoryBufferSource.cpp  \
liveMedia/ByteStreamMultiFileSource.cpp  \
liveMedia/DeviceSource.cpp  \
liveMedia/DigestAuthentication.cpp  \
liveMedia/DVVideoFileServerMediaSubsession.cpp  \
liveMedia/DVVideoRTPSink.cpp  \
liveMedia/DVVideoRTPSource.cpp  \
liveMedia/DVVideoStreamFramer.cpp  \
liveMedia/EBMLNumber.cpp  \
liveMedia/FileServerMediaSubsession.cpp  \
liveMedia/FileSink.cpp  \
liveMedia/FramedFileSource.cpp  \
liveMedia/FramedFilter.cpp  \
liveMedia/FramedSource.cpp  \
liveMedia/GenericMediaServer.cpp  \
liveMedia/GSMAudioRTPSink.cpp  \
liveMedia/H261VideoRTPSource.cpp  \
liveMedia/H263plusVideoFileServerMediaSubsession.cpp  \
liveMedia/H263plusVideoRTPSink.cpp  \
liveMedia/H263plusVideoRTPSource.cpp  \
liveMedia/H263plusVideoStreamFramer.cpp  \
liveMedia/H263plusVideoStreamParser.cpp  \
liveMedia/H264or5VideoFileSink.cpp  \
liveMedia/H264or5VideoRTPSink.cpp  \
liveMedia/H264or5VideoStreamDiscreteFramer.cpp  \
liveMedia/H264or5VideoStreamFramer.cpp  \
liveMedia/H264VideoFileServerMediaSubsession.cpp  \
liveMedia/H264VideoFileSink.cpp  \
liveMedia/H264VideoRTPSink.cpp  \
liveMedia/H264VideoRTPSource.cpp  \
liveMedia/H264VideoStreamDiscreteFramer.cpp  \
liveMedia/H264VideoStreamFramer.cpp  \
liveMedia/H265VideoFileServerMediaSubsession.cpp  \
liveMedia/H265VideoFileSink.cpp  \
liveMedia/H265VideoRTPSink.cpp  \
liveMedia/H265VideoRTPSource.cpp  \
liveMedia/H265VideoStreamDiscreteFramer.cpp  \
liveMedia/H265VideoStreamFramer.cpp  \
liveMedia/InputFile.cpp  \
liveMedia/JPEGVideoRTPSink.cpp  \
liveMedia/JPEGVideoRTPSource.cpp  \
liveMedia/JPEGVideoSource.cpp  \
liveMedia/Locale.cpp  \
liveMedia/MatroskaDemuxedTrack.cpp  \
liveMedia/MatroskaFile.cpp  \
liveMedia/MatroskaFileParser.cpp  \
liveMedia/MatroskaFileServerDemux.cpp  \
liveMedia/MatroskaFileServerMediaSubsession.cpp  \
liveMedia/Media.cpp  \
liveMedia/MediaSession.cpp  \
liveMedia/MediaSink.cpp  \
liveMedia/MediaSource.cpp  \
liveMedia/MP3ADU.cpp  \
liveMedia/MP3ADUdescriptor.cpp  \
liveMedia/MP3ADUinterleaving.cpp  \
liveMedia/MP3ADURTPSink.cpp  \
liveMedia/MP3ADURTPSource.cpp  \
liveMedia/MP3ADUTranscoder.cpp  \
liveMedia/MP3AudioFileServerMediaSubsession.cpp  \
liveMedia/MP3AudioMatroskaFileServerMediaSubsession.cpp  \
liveMedia/MP3FileSource.cpp  \
liveMedia/MP3Internals.cpp  \
liveMedia/MP3InternalsHuffman.cpp  \
liveMedia/MP3InternalsHuffmanTable.cpp  \
liveMedia/MP3StreamState.cpp  \
liveMedia/MP3Transcoder.cpp  \
liveMedia/MPEG1or2AudioRTPSink.cpp  \
liveMedia/MPEG1or2AudioRTPSource.cpp  \
liveMedia/MPEG1or2AudioStreamFramer.cpp  \
liveMedia/MPEG1or2Demux.cpp  \
liveMedia/MPEG1or2DemuxedElementaryStream.cpp  \
liveMedia/MPEG1or2DemuxedServerMediaSubsession.cpp  \
liveMedia/MPEG1or2FileServerDemux.cpp  \
liveMedia/MPEG1or2VideoFileServerMediaSubsession.cpp  \
liveMedia/MPEG1or2VideoRTPSink.cpp  \
liveMedia/MPEG1or2VideoRTPSource.cpp  \
liveMedia/MPEG1or2VideoStreamDiscreteFramer.cpp  \
liveMedia/MPEG1or2VideoStreamFramer.cpp  \
liveMedia/MPEG2IndexFromTransportStream.cpp  \
liveMedia/MPEG2TransportFileServerMediaSubsession.cpp  \
liveMedia/MPEG2TransportStreamAccumulator.cpp  \
liveMedia/MPEG2TransportStreamFramer.cpp  \
liveMedia/MPEG2TransportStreamFromESSource.cpp  \
liveMedia/MPEG2TransportStreamFromPESSource.cpp  \
liveMedia/MPEG2TransportStreamIndexFile.cpp  \
liveMedia/MPEG2TransportStreamMultiplexor.cpp  \
liveMedia/MPEG2TransportStreamTrickModeFilter.cpp  \
liveMedia/MPEG2TransportUDPServerMediaSubsession.cpp  \
liveMedia/MPEG4ESVideoRTPSink.cpp  \
liveMedia/MPEG4ESVideoRTPSource.cpp  \
liveMedia/MPEG4GenericRTPSink.cpp  \
liveMedia/MPEG4GenericRTPSource.cpp  \
liveMedia/MPEG4LATMAudioRTPSink.cpp  \
liveMedia/MPEG4LATMAudioRTPSource.cpp  \
liveMedia/MPEG4VideoFileServerMediaSubsession.cpp  \
liveMedia/MPEG4VideoStreamDiscreteFramer.cpp  \
liveMedia/MPEG4VideoStreamFramer.cpp  \
liveMedia/MPEGVideoStreamFramer.cpp  \
liveMedia/MPEGVideoStreamParser.cpp  \
liveMedia/MultiFramedRTPSink.cpp  \
liveMedia/MultiFramedRTPSource.cpp  \
liveMedia/OggDemuxedTrack.cpp  \
liveMedia/OggFile.cpp  \
liveMedia/OggFileParser.cpp  \
liveMedia/OggFileServerDemux.cpp  \
liveMedia/OggFileServerMediaSubsession.cpp  \
liveMedia/OggFileSink.cpp  \
liveMedia/OnDemandServerMediaSubsession.cpp  \
liveMedia/ourMD5.cpp  \
liveMedia/OutputFile.cpp  \
liveMedia/PassiveServerMediaSubsession.cpp  \
liveMedia/ProxyServerMediaSession.cpp  \
liveMedia/QCELPAudioRTPSource.cpp  \
liveMedia/QuickTimeFileSink.cpp  \
liveMedia/QuickTimeGenericRTPSource.cpp  \
liveMedia/RTCP.cpp  \
liveMedia/rtcp_from_spec.c  \
liveMedia/RTPInterface.cpp  \
liveMedia/RTPSink.cpp  \
liveMedia/RTPSource.cpp  \
liveMedia/RTSPClient.cpp  \
liveMedia/RTSPCommon.cpp  \
liveMedia/RTSPRegisterSender.cpp  \
liveMedia/RTSPServer.cpp  \
liveMedia/RTSPServerRegister.cpp  \
liveMedia/RTSPServerSupportingHTTPStreaming.cpp  \
liveMedia/ServerMediaSession.cpp  \
liveMedia/SimpleRTPSink.cpp  \
liveMedia/SimpleRTPSource.cpp  \
liveMedia/SIPClient.cpp  \
liveMedia/StreamParser.cpp  \
liveMedia/StreamReplicator.cpp  \
liveMedia/T140TextRTPSink.cpp  \
liveMedia/TCPStreamSink.cpp  \
liveMedia/TextRTPSink.cpp  \
liveMedia/TheoraVideoRTPSink.cpp  \
liveMedia/TheoraVideoRTPSource.cpp  \
liveMedia/uLawAudioFilter.cpp  \
liveMedia/VideoRTPSink.cpp  \
liveMedia/VorbisAudioRTPSink.cpp  \
liveMedia/VorbisAudioRTPSource.cpp  \
liveMedia/VP8VideoRTPSink.cpp  \
liveMedia/VP8VideoRTPSource.cpp  \
liveMedia/VP9VideoRTPSink.cpp  \
liveMedia/VP9VideoRTPSource.cpp  \
liveMedia/WAVAudioFileServerMediaSubsession.cpp  \
liveMedia/WAVAudioFileSource.cpp  \
mediaServer/DynamicRTSPServer.cpp  \
mediaServer/live555MediaServer.cpp  \
UsageEnvironment/HashTable.cpp  \
UsageEnvironment/strDup.cpp  \
UsageEnvironment/UsageEnvironment.cpp  \



include $(BUILD_SHARED_LIBRARY)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中LOCAL_SRC_FILES的下面的一堆值可能会有问题，根据版本的不同类名也会有增删变化，所以如果不行，可以用下面代码自己生成，然后将生成的文件内容替换LOCAL_SRC_FILES的值:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class CreateLive555Index {

public static void main(String[] args) {
	File file =  new File(&quot;D:/project/AndroidSDL/app/src/main/jni&quot;);
	File outFile = new File(&quot;D:/project/AndroidSDL/app/src/main/jni/output.txt&quot;);
	try {
		PrintStream ps = new PrintStream(outFile);
		createDirIndex(file,ps,&quot;&quot;);
		ps.flush();
		ps.close();
	} catch (FileNotFoundException e) {
		e.printStackTrace();
	}
	
}

private static void createDirIndex(File file,PrintStream ps,String outputPath){
	File[] files = file.listFiles();
	for(File f : files){
		if(f.isDirectory()){
			System.out.println(outputPath+f.getName()+&quot;/---------------------------&quot;);
			createDirIndex(f,ps,outputPath+f.getName()+&quot;/&quot;);
		}else{
			if(f.getName().endsWith(&quot;.cpp&quot;) || f.getName().endsWith(&quot;.c&quot;)){
				System.out.println(outputPath);
				ps.append(outputPath + f.getName() + &quot;  \\\r\n&quot;);
			}
		}
	}
}


}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后只需要在jni目录下执行ndk-build即可编译&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>red5流媒体服务器的搭建以及Tomcat上的集成</title>
        <link>http://www.liuschen.com/2016/10/27/red5.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/27/red5.html</guid>
        <pubDate>Thu, 27 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section&quot;&gt;1.流媒体服务器的搭建&lt;/h1&gt;

&lt;p&gt;首先需要安装Java环境，此步略过，然后下载&lt;a href=&quot;https://github.com/Red5/red5-server/releases/tag/v1.0.7-RELEASE&quot;&gt;red5&lt;/a&gt;文件，解压得到一个类似tomcat的服务器，点击red5.bat（window下下载的zip文件中的）启动，在浏览器中访问http://localhost:5080/如果能访问到red5就证明服务已经启动了。&lt;/p&gt;

&lt;p&gt;首先第一个问题，点击此时网页中的install，跳转的应用列表为空，这里只是说明由于某些原因网站上的资源被墙了，或不提供了，这个显示的并不是本地资源。如果想要示例程序自己在网上找就好了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;修改IP地址和端口：red5目录下conf文件夹中的red5.properties，主要改的是HTTP和RTMP的地址，如果想要在主机外访问，将0.0.0.0改为自己局域网中或是对外网提供的IP地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如果自己webapps文件夹内没有应用，也就是没有示例程序，也可以从网上下载别人打包好的red5-1.0.0-webapps-demos.zip解压到里面，先把应用跑起来走通功能才不至于被打击主动性。里面的Demo主要用了oflaDemo和Publisher（直接在webapps是没有这个名字的程序的，需要访问主界面，点击demo连接到的界面才能看到），oflaDemo是一个点播程序，而Publisher是一个兼具发布与直播的flash程序&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这时候如果之前已经改过IP那么在demo里面连接的时候一定要把localhost改为自己设置的IP地址&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;然后此时就可以直播了，可以现在Publisher程序中测试一下，然后改为用手机客户端推流，之前的地址是固定的，如&lt;strong&gt;rtmp://ip地址/oflaDemo&lt;/strong&gt;，后面在加上就是推流的地址，只要不是太离谱，可以随便取，如2334,322df什么的，由客户端指定，完整就是&lt;strong&gt;rtmp://ip地址/oflaDemo/自己取得流名称&lt;/strong&gt;，然后再在另一端用一个有流媒体播放的播放器，输入同一个地址就可以播放了。&lt;/p&gt;

&lt;h1 id=&quot;red5tomcat&quot;&gt;2.将red5集成在tomcat上启动&lt;/h1&gt;

&lt;p&gt;red5据说原本就是在tomcat上跑的，后来分开，而下载的red5的目录结构像极了tomcat，里面也有webapps跑着小程序，一度让我非常困惑。想把red5集成到tomcat中但是网上说的要么年代久远，要么大部分繁琐非常，试了几种方式，大概出了个思路，即使用网上给出的war包&lt;strong&gt;red5-war-1.0-RC1.zip&lt;/strong&gt;，中间我下载了n个red5版本的程序，也渐渐没了耐心。最后发现red5-war-1.0-RC1.zip里面包含了移植需要的所有东西。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;一.验证&lt;/h2&gt;

&lt;p&gt;将red5-war-1.0-RC1.zip包解压后，把ROOT.war解压到tomcat目录下webapps对应的ROOT包中，启动tomcat，如果正常启动并且能够访问到red5界面，说明这个包能用。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;二.新建项目&lt;/h2&gt;

&lt;p&gt;在myeclipse中新建一个web工程。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;三.配置文件的移植&lt;/h2&gt;

&lt;p&gt;将red5-war-1.0-RC1\ROOT\WEB-INF\classes下的配置文件拷贝到工程对应的src目录下，工程中的web.xml用red5-war-1.0-RC1\ROOT\WEB-INF中对应的替代。&lt;/p&gt;

&lt;h2 id=&quot;lib&quot;&gt;四.lib文件的移植&lt;/h2&gt;

&lt;p&gt;red5-war-1.0-RC1\ROOT\WEB-INF\lib下的文件全部拷贝到项目对应的文件夹中，打开cmd,切换到red5-war-1.0-RC1\ROOT\WEB-INF\classes中，执行命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jar -cvf red5final.jar org
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成了red5final.jar的库文件，同样拷贝到项目对应的lib文件夹中&lt;/p&gt;

&lt;p&gt;此时运行程序应该不会有错，我用了好几个版本的red5中的源程序来替代red5final.jar，但是总是会出各种问题。只有用同一个包中的才行。如果之前在tomcat上验证通过，这里也就不应该有什么问题。&lt;/p&gt;

&lt;h1 id=&quot;section-4&quot;&gt;3.通过推流和播放验证移植成功&lt;/h1&gt;

&lt;p&gt;首先，修改绑定的IP地址，若是找不到red5.properties或是类似的文件，就直接在red5-core.xml中修改，默认的rtmp端口一般是1935,搜一下就找到了。&lt;/p&gt;

&lt;p&gt;然后修改root-web.xml里面的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property name=&quot;contextPath&quot; value=&quot;/oflaDemo&quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;oflaDemo可以用其他字符，也是取一个名字，rtmp访问时会用到（紧跟在ip后面）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rtmp://ip地址/oflaDemo/自己取得流名称
&lt;/code&gt;&lt;/pre&gt;

</description>
      </item>
    
      <item>
        <title>Android手机视频直播推流-RTMP推流-解决方案一</title>
        <link>http://www.liuschen.com/2016/10/25/ffmpeg-jna.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/25/ffmpeg-jna.html</guid>
        <pubDate>Tue, 25 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;基于对原生视频编解码，也就是FFMPEG的学习，对音视频编解码有了初步的认识。最近在GITHUB上发现了一个开源的&lt;a href=&quot;https://github.com/beautifulSoup/RtmpRecoder/tree/master&quot;&gt;直播推流项目&lt;/a&gt;,用的同样是FFMPEG库，可是和我经常调用的方法不太一样，所以记录下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;jnijna&quot;&gt;1.JNI和JNA&lt;/h1&gt;

&lt;p&gt;JNI是&lt;strong&gt;Java Native Interface&lt;/strong&gt;的缩写，而JNA则是&lt;strong&gt;jna:Java Native Access&lt;/strong&gt;的缩写&lt;/p&gt;

&lt;p&gt;技术总是共通的，jni是Java和c/c++交互的技术，而jna则是在jni基础上，通过在一个java接口中描述目标native library的函数与结构，JNA将自动实现Java接口到native function的映射。从而消除了Java调用本地代码的隔阂，效率当然不如前者，并且只能单向调用，但是却不用编写本地代码，便利了开发。&lt;/p&gt;

&lt;h1 id=&quot;rtmprecoder&quot;&gt;2.RtmpRecoder分析&lt;/h1&gt;

&lt;p&gt;首先看其添加的依赖：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dependencies {
	compile fileTree(dir: &#39;libs&#39;, include: [&#39;*.jar&#39;])
	testCompile &#39;junit:junit:4.12&#39;
	compile &#39;com.android.support:appcompat-v7:23.1.1&#39;
   	 	compile &#39;com.android.support:design:23.1.1&#39;
	compile &#39;com.jakewharton:butterknife:7.0.1&#39;
	compile group: &#39;org.bytedeco&#39;, name: &#39;javacv&#39;, version: &#39;1.1&#39;
	compile group: &#39;org.bytedeco.javacpp-presets&#39;, name: &#39;opencv&#39;, version: &#39;3.0.0-1.1&#39;, classifier: &#39;android-arm&#39;
	compile group: &#39;org.bytedeco.javacpp-presets&#39;, name: &#39;opencv&#39;, version: &#39;3.0.0-1.1&#39;, classifier: &#39;android-x86&#39;
	compile group: &#39;org.bytedeco.javacpp-presets&#39;, name: &#39;ffmpeg&#39;, version: &#39;2.8.1-1.1&#39;, classifier: &#39;android-arm&#39;
	compile group: &#39;org.bytedeco.javacpp-presets&#39;, name: &#39;ffmpeg&#39;, version: &#39;2.8.1-1.1&#39;, classifier: &#39;android-x86&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;尤其是最后面的几项，项目里面用到了FFMPEG，但是却没有任何外部c或.so库的引用，那么就是说这些库包含在了这些Android官方提供的这些引用里面。&lt;/p&gt;

&lt;p&gt;同时，需要在gradle中的Android下加入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;packagingOptions {
    exclude &#39;META-INF/maven/org.bytedeco.javacpp-presets/opencv/pom.properties&#39;
    exclude &#39;META-INF/maven/org.bytedeco.javacpp-presets/opencv/pom.xml&#39;
    exclude &#39;META-INF/maven/org.bytedeco.javacpp-presets/ffmpeg/pom.properties&#39;
    exclude &#39;META-INF/maven/org.bytedeco.javacpp-presets/ffmpeg/pom.xml&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;通过之上的引用，在代码中用到的内容主要如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	//创建对象，传入直播推流地址以及帧的宽高信息
	FFmpegFrameRecorder recorder = new FFmpegFrameRecorder(ffmpeg_link, imageWidth, imageHeight, 1);
	//
    recorder.setVideoCodec(28);
	//设置编码格式，rmtp传输的是flv
    recorder.setFormat(&quot;flv&quot;);
    recorder.setSampleRate(sampleAudioRateInHz);
    // Set in the surface changed method,设置帧率
    recorder.setFrameRate(frameRate);
	
	recorder.start();
	
	recorder.stop();
    recorder.release();

	//设置时间戳
	long t = 1000 * (System.currentTimeMillis() - startTime);
    if (t &amp;gt; recorder.getTimestamp()) {
         recorder.setTimestamp(t);
     }
	
	Frame yuvImage = new Frame(imageWidth, imageHeight, Frame.DEPTH_UBYTE, 2);
	//将从onPreviewFrame回调中获取的帧数据data传入
	((ByteBuffer) yuvImage.image[0].position(0)).put(data);
	recorder.record(yuvImage);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中同时也有录音的功能&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class AudioRecordRunnable implements Runnable {

    @Override
    public void run() {
        android.os.Process.setThreadPriority(android.os.Process.THREAD_PRIORITY_URGENT_AUDIO);

        // Audio
        int bufferSize;
        ShortBuffer audioData;
        int bufferReadResult;

        bufferSize = AudioRecord.getMinBufferSize(sampleAudioRateInHz,
                AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT);
        audioRecord = new AudioRecord(MediaRecorder.AudioSource.MIC, sampleAudioRateInHz,
                AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT, bufferSize);

        
        audioData = ShortBuffer.allocate(bufferSize);

        Log.d(LOG_TAG, &quot;audioRecord.startRecording()&quot;);
        audioRecord.startRecording();

        /* ffmpeg_audio encoding loop */
        while (runAudioThread) {
            if (RECORD_LENGTH &amp;gt; 0) {
                audioData = samples[samplesIndex++ % samples.length];
                audioData.position(0).limit(0);
            }
            //Log.v(LOG_TAG,&quot;recording? &quot; + recording);
            bufferReadResult = audioRecord.read(audioData.array(), 0, audioData.capacity());
            audioData.limit(bufferReadResult);
            if (bufferReadResult &amp;gt; 0) {
                Log.v(LOG_TAG, &quot;bufferReadResult: &quot; + bufferReadResult);
                // If &quot;recording&quot; isn&#39;t true when start this thread, it never get&#39;s set according to this if statement...!!!
                // Why?  Good question...
                if (recording) {
                    try {
                        recorder.recordSamples(audioData);
                        //Log.v(LOG_TAG,&quot;recording &quot; + 1024*i + &quot; to &quot; + 1024*i+1024);
                    } catch (FFmpegFrameRecorder.Exception e) {
                        Log.v(LOG_TAG, e.getMessage());
                        e.printStackTrace();
                    }
                }
            }
        }
        Log.v(LOG_TAG, &quot;AudioThread Finished, release audioRecord&quot;);

        /* encoding finish, release recorder */
        if (audioRecord != null) {
            audioRecord.stop();
            audioRecord.release();
            audioRecord = null;
            Log.v(LOG_TAG, &quot;audioRecord released&quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看FFmpegFrameRecorder.java中的代码可以找到一些明显是ffmpeg中的函数，如av_register_all()，点击进入到了avformat.java，而avformat.java是avformat.h的映射，可见这里用的是JNA技术。&lt;/p&gt;

</description>
      </item>
    
      <item>
        <title>基于Android的手机推流</title>
        <link>http://www.liuschen.com/2016/10/17/ffmpeg.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/17/ffmpeg.html</guid>
        <pubDate>Mon, 17 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;这个推流器卡了很久，基于大神的代码，但是编译的时候总是出错，ffmpeg用的版本是3.1.4,有些函数提示不用了。有些很顺利就找到了替代的函数，有些则郁闷了很久，也正因为此，才逼自己去理解其中的含义。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;javac&quot;&gt;1.将java中获取到的字符串转换成c语言中的字符数组&lt;/h2&gt;

&lt;p&gt;strcpy(input_str,(*env)-&amp;gt;GetStringUTFChars(env,input-jstr, NULL));&lt;/p&gt;

&lt;p&gt;strcpy(output_str,(*env)-&amp;gt;GetStringUTFChars(env,output-jstr, NULL));&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;2.初始化组件&lt;/h2&gt;

&lt;p&gt;av-register-all();&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;3.如果有网络，初始化&lt;/h2&gt;

&lt;p&gt;avformat-network-init();&lt;/p&gt;

&lt;h2 id=&quot;ifmt-ctxavformatcontext&quot;&gt;4.打开输入函数ifmt-ctx为AVFormatContext对象，里面存储流以及参数信息&lt;/h2&gt;

&lt;p&gt;avformat-open-input(&amp;amp;ifmt-ctx, input-str, 0, 0)&lt;/p&gt;

&lt;h2 id=&quot;ifmt-ctx&quot;&gt;5.将流的信息写入ifmt-ctx&lt;/h2&gt;

&lt;p&gt;avformat-find-stream-info(ifmt-ctx, 0)&lt;/p&gt;

&lt;h2 id=&quot;avformatcontext&quot;&gt;6.获取输出流的AVFormatContext对象，计算出输出方式&lt;/h2&gt;

&lt;p&gt;avformat-alloc-output-context2(&amp;amp;ofmt-ctx, NULL, “flv”,output-str);&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;7.然后是流交换&lt;/h2&gt;

&lt;p&gt;需要将输入流对应的AVFormatContext中的信息写入到输出流中，AVFormatContext中有一个流数组，存储的流如音频流，视频流等等。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for (i = 0; i &amp;lt; ifmt_ctx-&amp;gt;nb_streams; i++) {
    AVStream *in_stream = ifmt_ctx-&amp;gt;streams[i];
    //通过id获取到对应解码器
    pCodec = avcodec_find_decoder(ofmt-&amp;gt;video_codec);
	//avformat_new_stream会新建流并把新的流存储在对应streams数组中
    AVStream *out_stream = avformat_new_stream(ofmt_ctx, pCodec);
    if (!out_stream) {
        LOGE( &quot;Failed allocating  output stream\n&quot;);
        ret = AVERROR_UNKNOWN;
        goto end;
     }
	//avcodec_copy_context，
     avcodec_parameters_copy(out_stream-&amp;gt;codecpar,in_stream-&amp;gt;codecpar);
     
     out_stream-&amp;gt;codecpar-&amp;gt;codec_tag = 0;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;avcodec_parameters_copy，这个是最坑爹的函数，原来大神的代码是avcodec_copy_context，我在avcodec.h中找到函数说的是&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;If you need to transfer the stream parameters from one codec context
 	to another, use an intermediate AVCodecParameters instance and the
 	avcodec_parameters_from_context() / avcodec_parameters_to_context()
 	functions.*/
attribute_deprecated
int avcodec_copy_context(AVCodecContext *dest, const AVCodecContext *src);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;结果完全被误导了，结果就是avformat_write_header一直返回-22，参数异常&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;8.打开流&lt;/h2&gt;

&lt;p&gt;avio-open2(&amp;amp;ofmt-ctx-&amp;gt;pb, output-str, AVIO-FLAG-WRITE,NULL,NULL);&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;9.写入头文件&lt;/h2&gt;

&lt;p&gt;avformat-write-header(ofmt-ctx, NULL);&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;10.往外写数据&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;while (1) {
      AVStream *in_stream, *out_stream;
      //Get an AVPacket
      ret = av_read_frame(ifmt_ctx, &amp;amp;pkt);
      if (ret &amp;lt; 0){
      LOGE( &quot;av_read_frame_break%d---%s\n&quot;,ret,av_err2str(ret));
      break;
      }

      //FIX：No PTS (Example: Raw H.264)
      //Simple Write PTS，如果没有pts值得话需要设置，pts是显示时间戳，dts是解码是解码时间戳
      if(pkt.pts==AV_NOPTS_VALUE){
          //Write PTS
          LOGE( &quot;AV_NOPTS_VALUE\n&quot;);
          AVRational time_base1=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;
          //Duration between 2 frames (us)
          int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;r_frame_rate);
          //Parameters
          pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);
          pkt.dts=pkt.pts;
          pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);
      }
      //Important:Delay，为了防止输出过快，加入的延时，是以视频帧为单位的
      if(pkt.stream_index==videoindex){
          AVRational time_base=ifmt_ctx-&amp;gt;streams[videoindex]-&amp;gt;time_base;
          LOGE( &quot;time_base==%d\n&quot;,pkt.stream_index);
          AVRational time_base_q={1,AV_TIME_BASE};
          int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);
          int64_t now_time = av_gettime() - start_time;
          LOGE( &quot;new_time%lld-----pts_time%lld\n&quot;,now_time,pts_time);
          if (pts_time &amp;gt; now_time)
              av_usleep(pts_time - now_time);
      }

      in_stream  = ifmt_ctx-&amp;gt;streams[pkt.stream_index];
      out_stream = ofmt_ctx-&amp;gt;streams[pkt.stream_index];

      LOGE( &quot;pkt.stream_index==%d\n&quot;,pkt.stream_index);

      /* copy packet */
      //Convert PTS/DTS
      LOGE(&quot;--------pkt.pts:%lld----pkt.dts:%lld----pkt.duration:%lld\n&quot;,pkt.pts,pkt.dts,pkt.duration);
      pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);
      pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base, AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX);
      pkt.duration = av_rescale_q(pkt.duration, in_stream-&amp;gt;time_base, out_stream-&amp;gt;time_base);
      pkt.pos = -1;

      LOGE(&quot;pkt.pts:%lld----pkt.dts:%lld----pkt.duration:%lld\n&quot;,pkt.pts,pkt.dts,pkt.duration);
      //Print to Screen
      if(pkt.stream_index==videoindex){
          LOGE(&quot;Send %8d video frames to output URL\n&quot;,pkt.stream_index);
          frame_index++;
        }

	//写入数据，关键函数
      ret = av_write_frame(ofmt_ctx, &amp;amp;pkt);
      //ret = av_interleaved_write_frame(ofmt_ctx, &amp;amp;pkt);

      if (ret &amp;lt; 0) {
      LOGE( &quot;Error muxing packet%d---%s\n&quot;,ret,av_err2str(ret));
          break;
      }
      av_packet_unref(&amp;amp;pkt);

  }
  //Write file trailer
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-6&quot;&gt;11.写入文件尾，推流结束&lt;/h2&gt;

&lt;p&gt;av-write-trailer(ofmt-ctx);&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;12.最后关闭流，释放资源&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;avformat_close_input(&amp;amp;ifmt_ctx);
  /* close output */
  if (ofmt_ctx &amp;amp;&amp;amp; !(ofmt-&amp;gt;flags &amp;amp; AVFMT_NOFILE))
      avio_close(ofmt_ctx-&amp;gt;pb);
  avformat_free_context(ofmt_ctx);
  if (ret &amp;lt; 0 &amp;amp;&amp;amp; ret != AVERROR_EOF) {
      LOGE( &quot;Error occurred.\n&quot;);
      return -1;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;psavformat-write-headerofmt-ctx-null-22avwriteframeofmtctx-pkt-22copyavformatcontextptsdts&quot;&gt;PS:遇到的问题主要有两个，一个是avformat-write-header(ofmt-ctx, NULL)返回-22，一个是av_write_frame(ofmt_ctx, &amp;amp;pkt)返回-22，前者是由于copy参数的函数没有找对，后者则是由于AVFormatContext中的关系没有理清，导致音频缺失，写入数据时也没有筛选,导致后面音频帧的pts,和dts数据错误，写入数据时报参数异常。&lt;/h3&gt;

&lt;h3 id=&quot;ffmpeghttpblogcsdnnetleixiaohua1020articledetails39803457&quot;&gt;参考：&lt;a href=&quot;http://blog.csdn.net/leixiaohua1020/article/details/39803457&quot;&gt;最简单的基于FFmpeg的推流器&lt;/a&gt;&lt;/h3&gt;
</description>
      </item>
    
      <item>
        <title>音视频编解码-ffmpeg解码</title>
        <link>http://www.liuschen.com/2016/10/12/ffmpeg-decode.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/12/ffmpeg-decode.html</guid>
        <pubDate>Wed, 12 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每次编解码必须先进行初始化组件av-register-all()，如果需要网络还需要avformat-network-init()；&lt;/p&gt;

&lt;p&gt;avformat-alloc-context()返回一个指向&lt;strong&gt;AVFormatContext&lt;/strong&gt;的指针，&lt;strong&gt;AVFormatContext&lt;/strong&gt;是一个用于处理封装格式的结构体；&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AVStream，AVCodecContext&lt;/strong&gt;视音频流对应的结构体，用于视音频编解码。&lt;/p&gt;

&lt;p&gt;AVFormatContext里面存储有一个流的list&lt;avstream&gt;,而AVStream里面存储有一个AVCodecContext，不过在我用这个版本显示&lt;/avstream&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/**
 * @deprecated use the codecpar struct instead
 */
attribute_deprecated
AVCodecContext *codec;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编译的时候也警告说是过时了，要用codecpar替代，而codecpar是一个AVCodecParameters类型的指针&lt;/p&gt;

&lt;p&gt;所有改完，生成.so文件不再报错后，部署到手机上，运行又报错：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JNI DETECTED ERROR IN APPLICATION: invalid reference returned from java.lang.String luofeng.com.ffmpegdemo.FFmpegUtils.getFFmpegInfo1()
09-28 10:27:14.363 18198-18198/luofeng.com.ffmpegdemo A/art: art/runtime/check_jni.cc:65]     from java.lang.String luofeng.com.ffmpegdemo.FFmpegUtils.getFFmpegInfo1()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是由于我开的cmd命令行目录占据着build文件夹，导致makeclean也无法重新build程序，也就是说新的改动没有生效。&lt;/p&gt;

&lt;p&gt;然后运行后又报错，是日志报的错avformat_open_input返回值不对，找答案找不到，最后发现忘了添加读写内存卡的权限。添加之后正常解码出了视频文件为YUV视频帧序列。&lt;/p&gt;

&lt;p&gt;在一些步骤返回错误码，若要知道错误码的错误类型可以用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	char buf2[500] = {0};
     av_strerror(ret, buf2, 1024);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将错误内容查询出来，再在log中输出即可。&lt;/p&gt;

&lt;h3 id=&quot;android-httpblogcsdnnetleixiaohua1020articledetails47010637&quot;&gt;参考：&lt;a href=&quot;http://blog.csdn.net/leixiaohua1020/article/details/47010637&quot;&gt;Android 视频解码器&lt;/a&gt;&lt;/h3&gt;

</description>
      </item>
    
      <item>
        <title>荒原日记-时空狂想</title>
        <link>http://www.liuschen.com/2016/10/02/wildernessdiary-time.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/10/02/wildernessdiary-time.html</guid>
        <pubDate>Sun, 02 Oct 2016 00:00:00 +0800</pubDate>
        <description>&lt;p&gt;我们存在于此的客观因素是时间和空间，人之于物，我想和物之于物没什么不同。只不过是人，进化出了高等的智能。所以有能力去认知自己的现状，进而改变其所处的环境。&lt;/p&gt;

&lt;p&gt;首先是唯心主义的观点，认为人才是世界的中心，世界是以人的意识为前提存在的，如果你认为荒诞，就不要再往下看了，先入为主往往会让人丧失判断力，“缸中之脑”其实就是一种唯心主义的现实体现，可怕的是你不觉得它是对的，却也无法证明它是错的。神学最初就是以唯心立论的，然而经过长达几个世纪科学的发展，它的观点不断的被现代科学否定，而它也像一个摇摆不定的娼妇，在一次次的改变着自己的主张，接收着现代科学的渗透。但是却一直未遭到废弃，究其原因，是因为现代科学解决不了人的信仰问题，自古以来，人类都对自己存在的意义充满了好奇。神学，宗教可以给你答案，然而科学给不了。所以现在仍有不少的科学家试图用科学的手段来证明一些宗教现象的背后的意义。&lt;/p&gt;

&lt;p&gt;称量灵魂重量就是一个典型的例子，有人说灵魂的重量是21g，事实上如何，怕也没人知道，只是觉得不可信，人有高矮胖瘦，更何况灵魂，只是说21g，无所谓严谨，说是科学实验恐怕也只是噱头。灵魂向来是一个抽象的概念，指的是我之所以是我的那个本质。久而久之，却连离体之说都被传的煞有其事。但是，灵魂究竟为何？是可以独立的还是某些硬件或是后天形成的特征。我们可以假设几个例子：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;灵魂是否存在于每个人的DNA中，如果是，那么同卵的双胞胎是否共享同样的灵魂？但现实生活中，他(他)们确是不同的个体 。&lt;/li&gt;
  &lt;li&gt;那么，灵魂不是存在于自己的基因当中，会不会存在于自己的记忆当中，当自身发育不完全或是年龄小的时候可能正是形成灵魂的时候，但是无论什么时候，总是会有那么一个时期，是自己构筑自己自我意识的时候。而这个时候大部分人都不会记得。因为人生来就是一张白纸，而白纸没有ID，它可以是特定的人，也可以是任何人。然后在之上灵魂会自己形成。&lt;/li&gt;
  &lt;li&gt;如果克隆技术发展到一定水平，允许克隆自己，那么，当我们克隆一个自己，并把自己现有的所有记忆移植到他之上。那么，这里就会产生一个有趣的问题，如果我(本体)在克隆体记忆移植完成并苏醒前死掉，那么克隆体苏醒时里面会不会是我本体的意识，可以肯定的是至少在本体无恙时不会是本体的意识，因为本体还活着，这就尴尬了。这是终极的假设，会出现几种可能情况，一是记忆可以移植，但是本体死亡前，克隆体是不会苏醒的，这说明了意识在宇宙中是一种独特的东西，具有唯一性，并且超越时空的限制；二是记忆可以移植，但是成功移植的条件是本体会立即死亡，这也说明意识是一种独特的存在，具有唯一性，但并不能证明和时空的关联；三是记忆可以移植并且本体和克隆体都正常，这个是最自然的，同时也是让人最难以接受的，它可能说明人类的意识可能不过是人自己的错觉罢了。它和自然界中的其他事物一样，兴盛衰败，人小了，懵懂，人老了，痴呆。都只不过是为了让人接受自然变化的平滑过渡而已。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;关于轮回，大部分都是宗教的观点和人美好的愿望，苦难者寄往来生，支配者渴望永生，然而不管是来生还是永生，还是我最初所想，都不是那么有趣，永生和死亡自古以来都是一个交缠的话题，我们害怕死亡，所以渴望永生，然而细细思量，无穷无尽的生命却更为恐怖。没有尽头，没有一个人能忍受的了，所以对于轮回的猜想才更为现实。事物有兴盛衰败的变化，这是自然界中最为合理的规则，所以人对永生的追求便寄望到了轮回一途。阴朝地府我想是不会有的，现今科学的发展也不容我这么考虑，然而轮回的存在也许是可能的，只不过并不是以我们期望的方式，如果意识是独特的存在，轮回可以说必然存在，如果意识不过是我们的错觉，我们并没有什么独特之处，那么我们死后，便可以轮回成任何人，我和你并没有什么区别，生来不过是一张白纸，我们的意识可能只不过是某些偶发因素导致的概率事件。当把发生概率的范围扩展至无穷（宇宙），那么即便是小概率事件也会变得相当大，而这，就是轮回。&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>Androidstudio解决javah生成头文件时找不到类的问题</title>
        <link>http://www.liuschen.com/2016/09/25/javah.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/09/25/javah.html</guid>
        <pubDate>Sun, 25 Sep 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;首先将命令行的工作目录切换到D:*&lt;strong&gt;*&lt;/strong&gt;\app\build\intermediates\classes\debug下，而不要切换到具体类的目录中，然后执行的命令是Javah (-classpath如果当前目录在别的目录可以跟这个，后面写上面所说路径) -jni 类名.文件名（只是文件名，不要加class扩展名）然后就会生成想要的头文件了。&lt;/p&gt;

&lt;p&gt;在目录中执行ndk-build的前提是该目录名必须是jni,否则会有如下异常：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Android NDK: Could not find application project directory !
Android NDK: Please define the NDK_PROJECT_PATH variable to point to it.
F:\programer\android-ndk-r10e\build/core/build-local.mk:143: *** Android NDK: Aborting    .  Stop.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Application.mk是配置编译的一些全局属性，最好还是写一下，如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;APP_PLATFORM = android-21
APP_ABI := armeabi-v7a armeabi
APP_STL := stlport_static
APP_OPTIM := debug

(1)APP_PLATFORM  使用的ndk库函数版本号。一般和SDK的版本相对应，各个版本在NDK目录下的platforms文件夹中

(2)APP_ABI  编译成什么类型的cpu的so, 拥有三个属性armeabi  armeabi-v7a  x86可以全选 也可以只用一个，如果全选也可以使用all.

(3)APP_STL       如何连接c++标准库 。
      stlport_static    静态链接 
      stlport_shared    动态链接 
      system    系统默认

(4)APP_OPTIM   编译版本，如果是DEBUG版本就会带上调试信息。可以使用gdb-server进行动态断点低调试。
      debug   调试版本    so中带调试信息
      release  发布版本   so不带调试信息
&lt;/code&gt;&lt;/pre&gt;

</description>
      </item>
    
      <item>
        <title>网络工程-抓包分析</title>
        <link>http://www.liuschen.com/2016/09/18/net-analysis.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/09/18/net-analysis.html</guid>
        <pubDate>Sun, 18 Sep 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;socket&quot;&gt;1.socket套接字&lt;/h2&gt;
&lt;p&gt;socket套接字是应用层调用的传输层提供的，传输层封装Tcp/Udp协议，网际层提供ip协议的封装。所以，一般情况下，调用套接字的层序默认的协议Tcp/Udp以及ip协议，http协议可以在应用层自己定制。&lt;/p&gt;

&lt;h2 id=&quot;retransmission&quot;&gt;2.Retransmission：&lt;/h2&gt;
&lt;p&gt;TCP协议是一个可靠的协议。它通过重新发送(retransmission)来实现TCP片段传输的可靠性。简单的说，TCP会不断重复发送TCP片段，直到片段被正确接收。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;3.协议：&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;* 网际层协议：包括：IP协议、ICMP协议（Ping）、ARP协议(用于广播发现)、RARP协议。
* 传输层协议：TCP协议、UDP协议。
* 应用层协议：FTP、Telnet、SMTP、HTTP、RIP、NFS、DNS
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;fiddlerwireshark&quot;&gt;4.fiddler和wireshark&lt;/h2&gt;
&lt;p&gt;fiddler可以监听抓取手机发送的包，是因为它本身具有&lt;strong&gt;代理功能&lt;/strong&gt;，手机通过这个代理的端口上网，fiddler监听这个代理端口从而抓取到手机的网络数据包，并且&lt;strong&gt;修改完监听的端口一定要重启应用才能生效&lt;/strong&gt;。wireshark则没有代理功能。所以单纯修改手机代理为同一局域网内电脑地址，并设置端口，在电脑中用wireshark虽然能抓到该端口有数据包，但是这些数据包是不能发送出去的，也就是说手机设置代理后是上不了网的，因为wireshark只负责抓包，没有代理功能。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;5.数据包类型含义：&lt;/h2&gt;

&lt;p&gt;SYN表示建立连接，&lt;/p&gt;

&lt;p&gt;FIN表示关闭连接，&lt;/p&gt;

&lt;p&gt;ACK表示响应，&lt;/p&gt;

&lt;p&gt;PSH表示有 DATA数据传输，&lt;/p&gt;

&lt;p&gt;RST表示连接重置。&lt;/p&gt;

&lt;h2 id=&quot;tcp-spurious-retransmission&quot;&gt;6.tcp spurious retransmission：&lt;/h2&gt;

&lt;h3 id=&quot;tcp&quot;&gt;tcp虚假重传&lt;/h3&gt;

&lt;p&gt;指实际上并没有超时，但看起来超时了，导致虚假超时重传的原因有很多种：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对于部分移动网络，当网络发生切换时会导致网络延时突增&lt;/li&gt;
  &lt;li&gt;当网络的可用带宽突然变小时，网络rtt会出现突增的情况，这会导致虚假超时重传&lt;/li&gt;
  &lt;li&gt;网络丢包（原始和重传的包都有可能丢包）会导致虚假重传超时。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;wireshark&quot;&gt;7.wireshark捕获的包：&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Frame：物理层的数据帧概况。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ethernet II：数据链路层以太网帧头部信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Internet Protocol Version 4：互联网层IP包头部信息。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transmission Control Protocol：传输层的数据段头部信息，此处是TCP协议。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hypertext Transfer Protocol：应用层的信息，此处是HTTP协议。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;socketwireshark&quot;&gt;8.socket通过wireshark捕获的包示例&lt;/h2&gt;
&lt;p&gt;通过socket正常建立tcp连接时，会先通过握手，然后发送消息，当找不到服务器时，会收到一个重置连接的包&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;正常连接的包&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/blog_wireshark_socket.png&quot; alt=&quot;正常连接情况&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;对应端口没有开服务，会直接被拒绝&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/blog_wireshark_disconnect.png&quot; alt=&quot;找不到服务器&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;找不到服务器(地址)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/not-find-server.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;finackclosewaitfinackfinackclosewait&quot;&gt;正常关闭(客户端发送关闭连接的包FIN_ACK,服务端收到后做相应关闭处理(参见下文CLOSE_WAIT)，然后发送FIN_ACK做最后确认,如果只有一个FIN_ACK包，就是服务端停在了CLOSE_WAIT状态，最后会发现保持了大量的连接)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/close-normal.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;异常关闭&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/close-error.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;netstat&quot;&gt;9.通过netstat命令得到的信息：&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://blog.csdn.net/kobejayandy/article/details/17655739&quot;&gt;引用出处&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;常用的三个状态是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ESTABLISHED 表示正在通信，&lt;/li&gt;
  &lt;li&gt;TIME_WAIT 表示主动关闭，&lt;/li&gt;
  &lt;li&gt;CLOSE_WAIT 表示被动关闭。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TCP协议规定，对于已经建立的连接，网络双方要进行四次握手才能成功断开连接，如果缺少了其中某个步骤，将会使连接处于假死状态，连接本身占用的资源不会被释放。网络服务器程序要同时管理大量连接，所以很有必要保证无用连接完全断开，否则大量僵死的连接会浪费许多服务器资源。在众多TCP状态中，最值得注意的状态有两个：CLOSE_WAIT和TIME_WAIT。&lt;/p&gt;

&lt;h3 id=&quot;timewait&quot;&gt;TIME_WAIT&lt;/h3&gt;

&lt;p&gt;TIME_WAIT 是主动关闭链接时形成的，等待2MSL时间，约4分钟。主要是防止最后一个ACK丢失。  由于TIME_WAIT 的时间会非常长，因此server端应尽量减少主动关闭连接&lt;/p&gt;

&lt;h3 id=&quot;closewait&quot;&gt;CLOSE_WAIT&lt;/h3&gt;

&lt;p&gt;CLOSE_WAIT是被动关闭连接是形成的。根据TCP状态机，服务器端收到客户端发送的FIN，则按照TCP实现发送ACK，因此进入CLOSE_WAIT状态。但如果服务器端不执行close()，就不能由CLOSE_WAIT迁移到LAST_ACK，则系统中会存在很多CLOSE_WAIT状态的连接。此时，可能是系统忙于处理读、写操作，而未将已收到FIN的连接，进行close。此时，recv/read已收到FIN的连接socket，会返回0。&lt;/p&gt;

&lt;p&gt;为什么需要 TIME_WAIT 状态？&lt;/p&gt;

&lt;p&gt;假设最终的ACK丢失，server将重发FIN，client必须维护TCP状态信息以便可以重发最终的ACK，否则会发送RST，结果server认为发生错误。TCP实现必须可靠地终止连接的两个方向(全双工关闭)，client必须进入 TIME_WAIT 状态，因为client可能面 临重发最终ACK的情形。&lt;br /&gt;
为什么 TIME_WAIT 状态需要保持 2MSL 这么长的时间？&lt;br /&gt;
如果 TIME_WAIT 状态保持时间不足够长(比如小于2MSL)，第一个连接就正常终止了。第二个拥有相同相关五元组的连接出现，而第一个连接的重复报文到达，干扰了第二个连接。TCP实现必须防止某个连接的重复报文在连接终止后出现，所以让TIME_WAIT状态保持时间足够长(2MSL)，连接相应方向上的TCP报文要么完全响应完毕，要么被 丢弃。建立第二个连接的时候，不会混淆。&lt;br /&gt;
 TIME_WAIT 和CLOSE_WAIT状态socket过多&lt;/p&gt;

&lt;p&gt;如果服务器出了异常，百分之八九十都是下面两种情况：&lt;br /&gt;
1.服务器保持了大量TIME_WAIT状态&lt;br /&gt;
2.服务器保持了大量CLOSE_WAIT状态，简单来说CLOSE_WAIT数目过大是由于被动关闭连接处理不当导致的。&lt;/p&gt;

&lt;h2 id=&quot;tcp-window-full&quot;&gt;10.TCP Window Full&lt;/h2&gt;

&lt;p&gt;tcp滑动窗口接收达到上限，此时会在响应包中告诉发送端，自己窗口达到上限，发送端就不再发送消息。&lt;/p&gt;

&lt;h3 id=&quot;wiresharkwin&quot;&gt;通过wireshark捕获的包,每条包信息后面的win就是本机向对方告知的本机接收窗口大小&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/tcp-window-full.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;单个包详情&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/tcp-window-full-details.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;糊涂窗口综合症&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/tcp-window-full-pic.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我这里遇到的问题是客户端向服务端发送包，服务端没有从滑动窗口及时取出数据，导致服务端接收窗口不断减小，从服务端TCP发回的响应信息就可以看出。最终，服务端接收窗口为零。客户端被告知服务端接收窗口为零，于是向服务端发送TCP window Full包，标志客户端不再发送正常数据包，开始进入零窗口探测模式，隔一段时间发送一个TCP ZerowindowProbe包，进行探测服务端是否能接收数据。&lt;/p&gt;

&lt;p&gt;参考资料:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/awpatp/archive/2013/02/17/2914152.html&quot;&gt;什么是TCP Window&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/olartan/p/4268269.html&quot;&gt;Windows系统下的TCP参数优化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/woaiyy/p/3554182.html&quot;&gt;TCP窗口滑动以及拥塞控制1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.chinaunix.net/uid-26548237-id-3966297.html&quot;&gt;TCP的流量控制与拥塞控制2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      </item>
    
      <item>
        <title>Android基站定位</title>
        <link>http://www.liuschen.com/2016/09/15/BLocation.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/09/15/BLocation.html</guid>
        <pubDate>Thu, 15 Sep 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;定位是一项精细的活，现在GPS基本上已经成为了手机上的一项基本功能，但同时它也有诸多的限制，如启动慢，受环境影响明显等，所以后来又有诸多辅助定位的手段，如基站定位，和WiFi定位，不过两者却都是需要基于庞大的数据支持，也就是必须有基站和WiFi热点与真实GPS坐标映射的数据库才行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;gsm--umts--lte&quot;&gt;1. GSM / UMTS / LTE&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt; MCC，Mobile Country Code，移动国家代码（中国的为460）；
 MNC，Mobile Network Code，移动网络号码（00移动 01联通 11电信4G）； 
 LAC/TAC(1~65535)，Location Area Code，位置区域码；
 CID/CI( 2G(1~65535), 3G/4G(1~268435455))，Cell Identity，基站编号；
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;android&quot;&gt;android手机的获取:&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;TelephonyManager manager = (TelephonyManager) getSystemService(Context.TELEPHONY_SERVICE);

 String operator = manager.getNetworkOperator();
 /**通过operator获取 MCC 和MNC */
 int MCC = Integer.parseInt(operator.substring(0, 3));
 int MNC = Integer.parseInt(operator.substring(3));

 GsmCellLocation location = (GsmCellLocation) manager.getCellLocation();

 /**通过GsmCellLocation获取中国移动和联通 LAC 和cellID */
 int LAC = location.getLac();
 int CID = location.getCid(); 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;cdma&quot;&gt;2. CDMA&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;BID(1~65535):cdmaCellLocation.getBaseStationId()（基站ID,同GSM的CID）
NID(0~65535):cdmaCellLocation.getNetworkId() (网络ID,同GSM的LAC)
SID(0~32767):cdmaCellLocation.getSystemId() (同GSM的MNC)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;android-1&quot;&gt;android手机中的获取&lt;/h3&gt;

 		TelephonyManager manager = (TelephonyManager) getSystemService(Context.TELEPHONY_SERVICE);&lt;br /&gt;
&lt;pre&gt;&lt;code&gt;    CdmaCellLocation location = (CdmaCellLocation)manager.getCellLocation();
    int BID = location.getBaseStationId();
    int NID = location.getNetworkId();
	int SID = location.getSystemId();
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section&quot;&gt;3. 手机信号的获取&lt;/h2&gt;

&lt;p&gt;Android可以通过以下代码获取到手机收到的所有信号&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TelephonyManager manager = (TelephonyManager) getSystemService(Context.TELEPHONY_SERVICE);
    String operator = manager.getNetworkOperator();
    /**通过operator获取 MCC 和MNC */
    int mcc = Integer.parseInt(operator.substring(0, 3));
    int mnc = Integer.parseInt(operator.substring(3));

    List&amp;lt;CellInfo&amp;gt; allCellInfo =  manager.getAllCellInfo();
    String result = &quot;&quot;;
    for (CellInfo info:allCellInfo) {

        if(info instanceof CellInfoCdma){
            CellIdentityCdma cellIdentityCdma = ((CellInfoCdma)info).getCellIdentity();
            result += &quot;BaseStationId:&quot;+cellIdentityCdma.getBasestationId()
                    +&quot;,NetworkId:&quot;+cellIdentityCdma.getNetworkId()+&quot;,SystemId：&quot;
                    +cellIdentityCdma.getSystemId()+&quot;,signal&quot;+((CellInfoCdma)info).getCellSignalStrength()+&quot;\n&quot;;
        }else if(info instanceof CellInfoLte){
            CellIdentityLte cellIdentityLte = ((CellInfoLte)info).getCellIdentity();
            result += &quot;Mnc:&quot;+cellIdentityLte.getMnc()
                    +&quot;,Ci:&quot;+cellIdentityLte.getCi()+&quot;,Tac：&quot;
                    +cellIdentityLte.getTac()+&quot;,signal&quot;+((CellInfoLte)info).getCellSignalStrength()+&quot;\n&quot;;
        }else if(info instanceof CellInfoGsm){
            CellIdentityGsm cellIdentityGsm = ((CellInfoGsm)info).getCellIdentity();
            result += &quot;Mnc:&quot;+cellIdentityGsm.getMnc()
                    +&quot;,Cid:&quot;+cellIdentityGsm.getCid()+&quot;,Lac：&quot;
                    +cellIdentityGsm.getLac()+&quot;,signal&quot;+((CellInfoGsm)info).getCellSignalStrength()+&quot;\n&quot;;
        }else if(info instanceof CellInfoWcdma){
            CellIdentityWcdma cellIdentityWcdma = ((CellInfoWcdma)info).getCellIdentity();
            result += &quot;Mnc:&quot;+cellIdentityWcdma.getMnc()
                    +&quot;,Cid:&quot;+cellIdentityWcdma.getCid()+&quot;,Lac：&quot;
                    +cellIdentityWcdma.getLac()+&quot;,signal&quot;+((CellInfoWcdma)info).getCellSignalStrength()+&quot;\n&quot;;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个if条件中得到的结果前面三个大抵是每个基站的标识以及区域信息，就如上面的MNC,CID之类的，而最后一个是获取的信号强度SignalStrength，第一个CellInfoCdma能获取到这几个值&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private int mCdmaDbm;   // This value is the RSSI value
private int mCdmaEcio;  // This value is the Ec/Io
private int mEvdoDbm;   // This value is the EVDO RSSI value
private int mEvdoEcio;  // This value is the EVDO Ec/Io
private int mEvdoSnr;   // Valid values are 0-8.  8 is the highest signal to noise ratio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;RSSI&lt;/strong&gt;:Received Signal Strength Indication接收的信号强度指示，无线发送层的可选部分，用来判定链接质量，以及是否增大广播发送强度，可用来测算距离&lt;br /&gt;
&lt;a href=&quot;http://www.cnblogs.com/lele/articles/2832885.html&quot;&gt;RSSI为什么是负值&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ec/Io&lt;/strong&gt;:这是一个反映手机端当前接收的导频信号（Pilot）的水平。手机开机首先做的事情就是搜索导频信号，如果搜索不到有用的导频信号，手机就无法正确识别网络。很多时候，手机经常会处在很多基站重叠覆盖的区域，也就是有很多导频的区域。各个导频之间也会相互干扰，形成导频污染。Ec表示手机当前接收到的可用导频信号强度，Io表示手机当前所接收到的所有干扰信号强度。所以，Ec/Io就表明手机当前所接收到的有用信号和干扰信号的比例。反映了手机在这一点上多路导频信号的整体覆盖水平。&lt;/p&gt;

&lt;p&gt;第二个CellInfoLte能获取到的值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private int mSignalStrength;  //信号强度
private int mRsrp;			  //表示LTE参考信号接收质量，这种度量主要是根据信号质量来对不同LTE候选小区进行排序。这种测量用作切换和小区重选决定的输入。
private int mRsrq;            //表示发送信号质量
private int mRssnr;
private int mCqi;
private int mTimingAdvance;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第三个CellSignalStrengthGsm和第四个CellSignalStrengthWcdma能获取到的值：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private int mSignalStrength; // Valid values are (0-31, 99) as defined in TS 27.007 8.5
private int mBitErrorRate;   // bit error rate (0-7, 99) as defined in TS 27.007 8.5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这里可能会郁闷，lte,cdma,gsm这些是什么？这些是手机用到的通信技术，具体解释网上很多，而划分到三大运营商如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;中国移动，2G：GSM，3G：TD-SCDMA ,4G:TD-LTE&lt;/li&gt;
  &lt;li&gt;中国联通，2G：GSM，3G：WCDMA ,4G:FDD-LTE&lt;/li&gt;
  &lt;li&gt;中国电信，2G：CDMA（实际上相当于2.5G），3G：CDMA 2000 ,4G:FDD-LTE、TD-LTE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在Android内部区分信号类型是按照如下规则:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch (networkType) {
        case NETWORK_TYPE_GPRS:
        case NETWORK_TYPE_GSM:
        case NETWORK_TYPE_EDGE:
        case NETWORK_TYPE_CDMA:
        case NETWORK_TYPE_1xRTT:
        case NETWORK_TYPE_IDEN:
            return NETWORK_CLASS_2_G;
        case NETWORK_TYPE_UMTS:
        case NETWORK_TYPE_EVDO_0:
        case NETWORK_TYPE_EVDO_A:
        case NETWORK_TYPE_HSDPA:
        case NETWORK_TYPE_HSUPA:
        case NETWORK_TYPE_HSPA:
        case NETWORK_TYPE_EVDO_B:
        case NETWORK_TYPE_EHRPD:
        case NETWORK_TYPE_HSPAP:
        case NETWORK_TYPE_TD_SCDMA:
            return NETWORK_CLASS_3_G;
        case NETWORK_TYPE_LTE:
        case NETWORK_TYPE_IWLAN:
            return NETWORK_CLASS_4_G;
        default:
            return NETWORK_CLASS_UNKNOWN;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些信号类型是为了针对不同手机卡不同返回值做具体处理用的，但是返回的东西的用途确是一样的&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;基站标识，用来从已经知道的数据库中取出对应的卫星坐标，这个获取数据库是关键，网上有许多付费提供的接口，有些免费，却限制次数，但是所幸，我需要做基站定位的地方，区域有限，想取到这些位置可能覆盖到的基站信息可能并不难，只需要写一个程序，拿着手机在区域内到处走走，搜集到所有基站的信息，然后自己建个数据库即可（0），也可以用网上限制次数的免费接口，反正只用一次也足够，总之，方法很多。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当前位置距离基站的距离，这个对于我而言比较复杂，我能想到的有两种方法，一是通过电波到达时间来算距离，即TDOA定位，但是找不到获取对应信息的方法，只能放弃，二是通过信号强度来计算出自己距离基站的位置，而信号强度上面已经获取到了，只剩下计算的方法（1）（2）。获取到距离后通过三个或三个以上的基站就可以定位到当前位置。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（0）&lt;a href=&quot;http://www.gpsspg.com/bs.htm&quot;&gt;基站查询&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;（1）&lt;a href=&quot;http://www.cnblogs.com/magicboy110/archive/2010/12/10/1902741.html&quot;&gt;比较详细的参考&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;（2）&lt;a href=&quot;http://www.360doc.com/content/15/0725/11/8493019_487280425.shtml&quot;&gt;自由空间路径传播损耗&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;除了基站定位，还有WiFi定位，与基站定位原理一样，只要附近有WiFi，不需要连接上，只需获取其唯一的标识，然后通过数据库查询其对应的坐标即可，数据库的建立是问题，毕竟WiFi哪里都是，于是有人说，找几个出租，给些钱，装上GPS与WiFi连接对应设备到处跑，不是也很快吗，想想也是，一些看似庞大的工程，其实往往不难，只要用对地方，我们平时常用app，谁知道会不会把我们连接的WiFi和已经获取GPS坐标绑定上传呢。&lt;/p&gt;

&lt;p&gt;以上是Android基站定位的原理，其实如果不执着于完全的自主开发，完全可以用第三方提供的服务如&lt;a href=&quot;http://lbsyun.baidu.com/index.php?title=android-locsdk&quot;&gt;百度定位&lt;/a&gt;，大致原理相同，但是很多算法不需要自己去考虑。&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>netty point</title>
        <link>http://www.liuschen.com/2016/08/26/netty-point.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/08/26/netty-point.html</guid>
        <pubDate>Fri, 26 Aug 2016 00:00:00 +0800</pubDate>
        <description>&lt;h2 id=&quot;channelfuture&quot;&gt;1. ChannelFuture的作用&lt;/h2&gt;
&lt;p&gt;channelFuture 是为了保存channel异步操作的结果的，在Netty中所有的IO操作都是异步的，因此，你不能立刻得知消息是否被正确处理，但是我们可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过Future和ChannelFuture,他们可以注册一个监听，当操作执行成功或失败时监听会自动触发。总之，所有的操作都会返回一个ChannelFuture。&lt;/p&gt;

&lt;h2 id=&quot;ctxclose-and-ctxchannelcloseoverflowhttpstackoverflowcomquestions21240981in-netty-4-whats-the-difference-between-ctx-close-and-ctx-channel-close&quot;&gt;2. ctx.close and ctx.channel.close的区别（&lt;a href=&quot;http://stackoverflow.com/questions/21240981/in-netty-4-whats-the-difference-between-ctx-close-and-ctx-channel-close&quot;&gt;引用自overflow&lt;/a&gt;）&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s say we have three handlers in the pipeline, and they all intercept the close() operation, and calls ctx.close() in it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;ChannelPipeline p = ...;
p.addLast(&quot;A&quot;, new SomeHandler());
p.addLast(&quot;B&quot;, new SomeHandler());
p.addLast(&quot;C&quot;, new SomeHandler());
...

public class SomeHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void close(ChannelHandlerContext ctx, ChannelPromise promise) {
        ctx.close(promise);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Channel.close() will trigger C.close(), B.close(), A.close(), and then close the channel.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ChannelPipeline.context(“C”).close() will trigger B.close(), A.close(), and then close the channel.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ChannelPipeline.context(“B”).close() will trigger A.close(), and then close the channel.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ChannelPipeline.context(“A”).close() will close the channel. No handlers will be called.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;So, when you should use Channel.close() and ChannelHandlerContext.close()? The rule of thumb is:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you are writing a ChannelHandler and wanna close the channel in the handler, call ctx.close().&lt;br /&gt;
If you are closing the channel from outside the handler (e.g. you have a background thread which is not an I/O thread, and you want to close the connection from that thread.)&lt;/p&gt;

&lt;h2 id=&quot;channelpromise&quot;&gt;3. ChannelPromise的作用&lt;/h2&gt;

&lt;p&gt;Promise与future的不同在于它可以设置自己的状态，是在future之上的扩展。所以Promise是netty内部用到的，给返回给用户的对象Future是同一个。&lt;/p&gt;

&lt;h2 id=&quot;shutdowngracefully&quot;&gt;4. 优雅关闭shutdownGracefully&lt;/h2&gt;

&lt;p&gt;这是一个有效的关闭客户端/服务端的方法，会向对方发送结束连接请求，同时关闭信道和Pipeline，同时退出循环(结束线程等待，向下执行)。&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>netty框架是如何执行connect连接的</title>
        <link>http://www.liuschen.com/2016/08/25/netty_connect.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/08/25/netty_connect.html</guid>
        <pubDate>Thu, 25 Aug 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;section&quot;&gt;连接&lt;/h1&gt;

&lt;p&gt;以客户端为例，netty是通过以下代码发起服务请求的，其中b是Bootstrap的实例对象&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;future = b.connect(new InetSocketAddress(host, port)).sync();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而上面的connect方法是Bootstrap对象中的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public ChannelFuture connect(SocketAddress remoteAddress) {
    if(remoteAddress == null) {
        throw new NullPointerException(&quot;remoteAddress&quot;);
    } else {
        this.validate();
        return this.doConnect(remoteAddress, this.localAddress());
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上的doConnect对象是在同一个类中的方法，其中的ChannelFuture和PromiseFuture分别存储的是注册和连接的结果，和外部的对象有所区别，因为PromiseFuture是继承ChannelFuture的接口，其中多了一些设置的方法，这个方法是先初始化并且注册，然后如果成功的话，就开始进行连接操作，先做连 接代码的追踪&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private ChannelFuture doConnect(final SocketAddress remoteAddress, final SocketAddress localAddress) {
    final ChannelFuture regFuture = this.initAndRegister();
    final Channel channel = regFuture.channel();
    if(regFuture.cause() != null) {
        return regFuture;
    } else {
        final ChannelPromise promise = channel.newPromise();
        if(regFuture.isDone()) {
            doConnect0(regFuture, channel, remoteAddress, localAddress, promise);
        } else {
            regFuture.addListener(new ChannelFutureListener() {
                public void operationComplete(ChannelFuture future) throws Exception {
                    Bootstrap.doConnect0(regFuture, channel, remoteAddress, localAddress, promise);
                }
            });
        }

        return promise;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上的doConnect依然在同一个类中&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private static void doConnect0(final ChannelFuture regFuture, final Channel channel, final SocketAddress remoteAddress, final SocketAddress localAddress, final ChannelPromise promise) {
    channel.eventLoop().execute(new OneTimeTask() {
        public void run() {
            if(regFuture.isSuccess()) {
                if(localAddress == null) {
                    channel.connect(remoteAddress, promise);
                } else {
                    channel.connect(remoteAddress, localAddress, promise);
                }

                promise.addListener(ChannelFutureListener.CLOSE_ON_FAILURE);
            } else {
                promise.setFailure(regFuture.cause());
            }

        }
    });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后上面的connect方法是在channel接口中定义的方法，channel类继承关系如图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xpui7.com1.z0.glb.clouddn.com/blognetty_channel.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中NioSocketChannel的继承关系：&lt;code&gt;NioSocketChannel extends AbstractNioByteChannel implements SocketChannel &lt;/code&gt;而AbstractNioByteChannel又继承于AbstractNioChannel，在AbstractNioChannel中在找到了connect的实现方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public final void connect(final SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) {
        if(promise.setUncancellable() &amp;amp;&amp;amp; this.ensureOpen(promise)) {
            try {
                if(AbstractNioChannel.this.connectPromise != null) {
                    throw new IllegalStateException(&quot;connection attempt already made&quot;);
                }

                boolean t = AbstractNioChannel.this.isActive();
                if(AbstractNioChannel.this.doConnect(remoteAddress, localAddress)) {
                    this.fulfillConnectPromise(promise, t);
                } else {
                    AbstractNioChannel.this.connectPromise = promise;
                    AbstractNioChannel.this.requestedRemoteAddress = remoteAddress;
                    int connectTimeoutMillis = AbstractNioChannel.this.config().getConnectTimeoutMillis();
                    if(connectTimeoutMillis &amp;gt; 0) {
                        AbstractNioChannel.this.connectTimeoutFuture = AbstractNioChannel.this.eventLoop().schedule(new OneTimeTask() {
                            public void run() {
                                ChannelPromise connectPromise = AbstractNioChannel.this.connectPromise;
                                ConnectTimeoutException cause = new ConnectTimeoutException(&quot;connection timed out: &quot; + remoteAddress);
                                if(connectPromise != null &amp;amp;&amp;amp; connectPromise.tryFailure(cause)) {
                                    AbstractNioUnsafe.this.close(AbstractNioUnsafe.this.voidPromise());
                                }

                            }
                        }, (long)connectTimeoutMillis, TimeUnit.MILLISECONDS);
                    }

                    promise.addListener(new ChannelFutureListener() {
                        public void operationComplete(ChannelFuture future) throws Exception {
                            if(future.isCancelled()) {
                                if(AbstractNioChannel.this.connectTimeoutFuture != null) {
                                    AbstractNioChannel.this.connectTimeoutFuture.cancel(false);
                                }

                                AbstractNioChannel.this.connectPromise = null;
                                AbstractNioUnsafe.this.close(AbstractNioUnsafe.this.voidPromise());
                            }

                        }
                    });
                }
            } catch (Throwable var6) {
                promise.tryFailure(this.annotateConnectException(var6, remoteAddress));
                this.closeIfClosed();
            }

        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上的都Connect是由具体功能的实现类NioSocketChannel中实现具体方法的定义&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protected boolean doConnect(SocketAddress remoteAddress, SocketAddress localAddress) throws Exception {
    if(localAddress != null) {
        this.javaChannel().socket().bind(localAddress);
    }

    boolean success = false;

    boolean var5;
    try {
        boolean connected = this.javaChannel().connect(remoteAddress);
        if(!connected) {
            this.selectionKey().interestOps(8);
        }

        success = true;
        var5 = connected;
    } finally {
        if(!success) {
            this.doClose();
        }

    }

    return var5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终的承担者依旧是调用的Java中的nio来进行连接的&lt;code&gt;this.javaChannel().connect(remoteAddress)&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;section-1&quot;&gt;注册&lt;/h1&gt;

&lt;p&gt;然后继续之前的注册操作的代码分析，先是跳转到AbstractBootstrap中，在这里面实际是同过&lt;code&gt;ChannelFuture regFuture = this.group().register(channel)&lt;/code&gt;来生成一个该方法返回的ChannelFuture&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;final ChannelFuture initAndRegister() {
    Channel channel = this.channelFactory().newChannel();

    try {
        this.init(channel);
    } catch (Throwable var3) {
        channel.unsafe().closeForcibly();
        return (new DefaultChannelPromise(channel, GlobalEventExecutor.INSTANCE)).setFailure(var3);
    }

    ChannelFuture regFuture = this.group().register(channel);
    if(regFuture.cause() != null) {
        if(channel.isRegistered()) {
            channel.close();
        } else {
            channel.unsafe().closeForcibly();
        }
    }

    return regFuture;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而这个是接口EventLoopGroup中定义的方法，故名思义，EventLoopGroup就是一个集，或是池,其中继承关系如下所示：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;EventLoopGroup
    &lt;ul&gt;
      &lt;li&gt;MultithreadEventLoopGroup
        &lt;ul&gt;
          &lt;li&gt;LocalEventLoopGroup&lt;/li&gt;
          &lt;li&gt;EpollEventLoopGroup&lt;/li&gt;
          &lt;li&gt;NioEventLoopGroup&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;ThreadPerChannelEventLoopGroup
        &lt;ul&gt;
          &lt;li&gt;OioEventLoopGroup&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;EventLoop
        &lt;ul&gt;
          &lt;li&gt;SingleThreadEventLoop
            &lt;ul&gt;
              &lt;li&gt;EpollEventLoop&lt;/li&gt;
              &lt;li&gt;ThreadPerChannelEventLoop&lt;/li&gt;
              &lt;li&gt;LocalEventLoop&lt;/li&gt;
              &lt;li&gt;NioEventLoop&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;EmbeddedEventLoop&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;并且其中的group是我们在外部通过实例化&lt;code&gt;private EventLoopGroup group = new NioEventLoopGroup();&lt;/code&gt;创建的，&lt;/p&gt;

&lt;p&gt;在NioEventLoopGroup的父类MultithreadEventLoopGroup中找到了对应的实现方法，next返回的是一个单独的执行单元EventLoop；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public ChannelFuture register(Channel channel) {
    return this.next().register(channel);
}

public EventLoop next() {
    return (EventLoop)super.next();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由上面的继承关系可以看出，要找到 &lt;code&gt;this.next().register(channel)&lt;/code&gt;的具体实现需要在SingleThreadEventLoop和NioEventLoop中寻找，在SingleThreadEventLoop中找到了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public ChannelFuture register(Channel channel) {
    return this.register(channel, new DefaultChannelPromise(channel, this));
}

public ChannelFuture register(Channel channel, ChannelPromise promise) {
    if(channel == null) {
        throw new NullPointerException(&quot;channel&quot;);
    } else if(promise == null) {
        throw new NullPointerException(&quot;promise&quot;);
    } else {
        channel.unsafe().register(this, promise);
        return promise;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;TO BE CONTINUE&lt;/p&gt;
</description>
      </item>
    
      <item>
        <title>netty自带心跳响应</title>
        <link>http://www.liuschen.com/2016/08/20/netty-beat.html</link>
        <guid isPermaLink="true">http://www.liuschen.com/2016/08/20/netty-beat.html</guid>
        <pubDate>Sat, 20 Aug 2016 00:00:00 +0800</pubDate>
        <description>&lt;blockquote&gt;
  &lt;p&gt;文章链接:&lt;a href=&quot;http://www.liuschen.com&quot;&gt;http://www.liuschen.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;netty客户端的写法，不再赘述：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;b.group(group).channel(NioSocketChannel.class)
				.option(ChannelOption.TCP_NODELAY, true)
				.handler(new ChannelInitializer&amp;lt;SocketChannel&amp;gt;() {
					@Override
					protected void initChannel(SocketChannel ch) throws Exception {
						
						ch.pipeline().addLast(new IdleStateHandler(HEARTBEAT_READABLE_INTERVAL_TIME,
									HEARTBEAT_WRITABLE_INTERVAL_TIME, HEARTBEAT_READ_WRITE_INTERVAL_TIME));
							ch.pipeline().addLast(&quot;HeartBeatHandler&quot;, new HeartBeatHandler(client));

					}

				});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;IdleStateHandler类是为了监听空闲的状态，传入的参数分别为，读空闲，写空闲，以及读写空闲&lt;br /&gt;
而自定义的一个handler HeartBeatHandler则是为了复写其中的一个处理心跳的方法 userEventTriggered&lt;/p&gt;

&lt;h3 id=&quot;idlestatehandler&quot;&gt;IdleStateHandler类中有三个内部函数（任务函数）&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AllIdleTimeoutTask&lt;/li&gt;
  &lt;li&gt;ReaderIdleTimeoutTask&lt;/li&gt;
  &lt;li&gt;WriterIdleTimeoutTask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而通过每一个任务函数中实现IdleStateEvent状态的赋值以及向下的透传，最终传到HeartBeatHandler中处理:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;try {
         IdleStateEvent t;
         if(IdleStateHandler.this.firstWriterIdleEvent) {
         IdleStateHandler.this.firstWriterIdleEvent = false;
          t = IdleStateEvent.FIRST_WRITER_IDLE_STATE_EVENT;
             } else {
                        t = IdleStateEvent.WRITER_IDLE_STATE_EVENT;
                }

         IdleStateHandler.this.channelIdle(this.ctx, t);


protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) throws Exception {
    ctx.fireUserEventTriggered(evt);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至于三个超时任务的执行主要是通过时间戳的对比来判断的&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;读:&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
    if(this.readerIdleTimeNanos &amp;gt; 0L || this.allIdleTimeNanos &amp;gt; 0L) {
        this.lastReadTime = System.nanoTime();
        this.reading = false;
    }

    ctx.fireChannelReadComplete();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;section-1&quot;&gt;写:&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;this.writeListener = new ChannelFutureListener() {
        public void operationComplete(ChannelFuture future) throws Exception {
            IdleStateHandler.this.lastWriteTime = System.nanoTime();
            IdleStateHandler.this.firstWriterIdleEvent = IdleStateHandler.this.firstAllIdleEvent = true;
        }
    };
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在超时任务中，真正判断是否执行超时动作的是下面的函数（以读超时为例）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;long nextDelay = IdleStateHandler.this.readerIdleTimeNanos;
            if(!IdleStateHandler.this.reading) {
                nextDelay -= System.nanoTime() - IdleStateHandler.this.lastReadTime;
            }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;nextDelay：赋的值是用户设置的也就是传IdleStateHandler构造函数的的值，然后该值减去（当前时间-上次动作完成的时间），如果结果小于0就表示超时了。&lt;/p&gt;

&lt;p&gt;而读写超时略有不同，取的是上次读写完成时间中距离现在最近的时间为上次读写完成时间&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;long nextDelay = IdleStateHandler.this.allIdleTimeNanos;
            if(!IdleStateHandler.this.reading) {
                nextDelay -= System.nanoTime() - Math.max(IdleStateHandler.this.lastReadTime, IdleStateHandler.this.lastWriteTime);
            }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，超时任务是从handler添加时就已经开始执行的&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public void handlerAdded(ChannelHandlerContext ctx) throws Exception {
    if(ctx.channel().isActive() &amp;amp;&amp;amp; ctx.channel().isRegistered()) {
        this.initialize(ctx);
    }
}

private void initialize(ChannelHandlerContext ctx) {
    switch(this.state) {
    case 1:
    case 2:
        return;
    default:
        this.state = 1;
        EventExecutor loop = ctx.executor();
        this.lastReadTime = this.lastWriteTime = System.nanoTime();
        if(this.readerIdleTimeNanos &amp;gt; 0L) {
            this.readerIdleTimeout = loop.schedule(new IdleStateHandler.ReaderIdleTimeoutTask(ctx), this.readerIdleTimeNanos, TimeUnit.NANOSECONDS);
        }

        if(this.writerIdleTimeNanos &amp;gt; 0L) {
            this.writerIdleTimeout = loop.schedule(new IdleStateHandler.WriterIdleTimeoutTask(ctx), this.writerIdleTimeNanos, TimeUnit.NANOSECONDS);
        }

        if(this.allIdleTimeNanos &amp;gt; 0L) {
            this.allIdleTimeout = loop.schedule(new IdleStateHandler.AllIdleTimeoutTask(ctx), this.allIdleTimeNanos, TimeUnit.NANOSECONDS);
        }

    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;鉴于此，也就是说，IdleStateHandler在添加到handler链中时就已经开始执行它的超时任务，以设置的时间间隔为周期，不停的检查是否超时，一旦超时就会执行超时任务对应动作向下透传，在自定义的心跳handler中通过特定函数捕获&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@Override
public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception {
	if(IdleStateEvent.class.isAssignableFrom(evt.getClass())){
		IdleStateEvent event = (IdleStateEvent) evt;
		if(event.state() == IdleState.WRITER_IDLE){

		}else if(event.state() == IdleState.READER_IDLE){

		}else if (event.state() == IdleState.ALL_IDLE){
			
		}
	}
	ctx.fireUserEventTriggered(evt);
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      </item>
    
  </channel>
</rss>